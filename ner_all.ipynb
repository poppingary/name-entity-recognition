{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_all.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6nR5j3BMvuj2",
        "FEULQai2mWxe",
        "0mdEcxxVms26",
        "5INZ1yMBFt9I",
        "dQKEECZgJHx9",
        "AcmGrAALm3P8",
        "CqdFDlPynH8W",
        "4em_3j9tnCXE",
        "5qf9pIllnCXO",
        "cv929B6hr9QF",
        "WFXEXBqJecFM",
        "am2_t0dBs9QP",
        "KoXVmjLPXvp_",
        "u4G-SKCV02bC",
        "wtsl-5Z72B1S",
        "qaHUikHG2c8c",
        "WpHxDMXmUrDs",
        "bBtg0qvQcD3x",
        "f3EywXlSyLG-"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poppingary/name-entity-recognition/blob/main/ner_all.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A. Introduction\n",
        "This notebook includes all experiments and tests we did for name entity recognition (NER). We used 2 \"pretrained\" models which we downloaded through the official packages.\n",
        "\n",
        " 1. TagLM: The official AllenNLP package from [allennlp](https://demo.allennlp.org/named-entity-recognition/named-entity-recognition)\n",
        "\n",
        " 2. Flair: The official flair package from [flair](https://github.com/flairNLP/flair/blob/master/resources/docs/TUTORIAL_2_TAGGING.md). We used the \"ner-pooled\" model the same as the paper's model."
      ],
      "metadata": {
        "id": "o9dQtW5Bhr5T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nR5j3BMvuj2"
      },
      "source": [
        "# B. Test pretrained model\n",
        "The following code is for downloading and testing the pretrained model from the official package. We followed the official instructions and tested some sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TagLM"
      ],
      "metadata": {
        "id": "FEULQai2mWxe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install"
      ],
      "metadata": {
        "id": "0mdEcxxVms26"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsTybBUGvjSZ",
        "outputId": "5010c111-38cc-4e1f-c860-4fc00b9e2c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 6.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 21.1.3\n",
            "    Uninstalling pip-21.1.3:\n",
            "      Successfully uninstalled pip-21.1.3\n",
            "Successfully installed pip-21.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaqkAVHCvr4d",
        "outputId": "f4a4f873-1da9-4d19-87dd-a5de1aaa9969"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting allennlp==2.1.0\n",
            "  Downloading allennlp-2.1.0-py3-none-any.whl (585 kB)\n",
            "     |████████████████████████████████| 585 kB 6.6 MB/s            \n",
            "\u001b[?25hCollecting allennlp-models==2.1.0\n",
            "  Downloading allennlp_models-2.1.0-py3-none-any.whl (407 kB)\n",
            "     |████████████████████████████████| 407 kB 42.0 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (1.19.5)\n",
            "Collecting torch<1.8.0,>=1.6.0\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "     |████████████████████████████████| 776.8 MB 16 kB/s              \n",
            "\u001b[?25hCollecting transformers<4.4,>=4.1\n",
            "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "     |████████████████████████████████| 1.9 MB 28.1 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (3.6.4)\n",
            "Collecting filelock<3.1,>=3.0\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (4.62.3)\n",
            "Collecting jsonnet>=0.10.0\n",
            "  Downloading jsonnet-0.17.0.tar.gz (259 kB)\n",
            "     |████████████████████████████████| 259 kB 34.7 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "     |████████████████████████████████| 1.2 MB 46.9 MB/s            \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "     |████████████████████████████████| 124 kB 57.5 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (1.0.1)\n",
            "Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (2.23.0)\n",
            "Collecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.20.24-py3-none-any.whl (131 kB)\n",
            "     |████████████████████████████████| 131 kB 57.7 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (3.2.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (1.4.1)\n",
            "Requirement already satisfied: lmdb in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (0.99)\n",
            "Requirement already satisfied: spacy<3.1,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (2.2.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (3.1.0)\n",
            "Collecting jsonpickle\n",
            "  Downloading jsonpickle-2.0.0-py2.py3-none-any.whl (37 kB)\n",
            "Collecting torchvision<0.9.0,>=0.8.1\n",
            "  Downloading torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n",
            "     |████████████████████████████████| 12.8 MB 13.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from allennlp==2.1.0) (8.12.0)\n",
            "Collecting conllu==4.4\n",
            "  Downloading conllu-4.4-py2.py3-none-any.whl (15 kB)\n",
            "Collecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "     |████████████████████████████████| 56 kB 6.0 MB/s             \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "     |████████████████████████████████| 64 kB 3.5 MB/s             \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.24.0,>=1.23.24\n",
            "  Downloading botocore-1.23.24-py3-none-any.whl (8.4 MB)\n",
            "     |████████████████████████████████| 8.4 MB 46.2 MB/s            \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
            "     |████████████████████████████████| 79 kB 10.1 MB/s            \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.1.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.18->allennlp==2.1.0) (2.10)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (1.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (3.0.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (0.8.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (57.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX>=1.2->allennlp==2.1.0) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<1.8.0,>=1.6.0->allennlp==2.1.0) (3.10.0.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision<0.9.0,>=0.8.1->allennlp==2.1.0) (7.1.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp==2.1.0) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp==2.1.0) (4.8.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "     |████████████████████████████████| 895 kB 76.7 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<4.4,>=4.1->allennlp==2.1.0) (21.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "     |████████████████████████████████| 3.3 MB 45.5 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->allennlp-models==2.1.0) (0.2.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->allennlp==2.1.0) (1.5.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->allennlp==2.1.0) (1.15.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (21.2.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (1.11.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->allennlp==2.1.0) (0.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp==2.1.0) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->allennlp==2.1.0) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.24->boto3<2.0,>=1.14->allennlp==2.1.0) (2.8.2)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "     |████████████████████████████████| 127 kB 49.3 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<4.4,>=4.1->allennlp==2.1.0) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<4.4,>=4.1->allennlp==2.1.0) (3.0.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4.4,>=4.1->allennlp==2.1.0) (7.1.2)\n",
            "Building wheels for collected packages: overrides, jsonnet, word2number, ftfy\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=e95c0b733cc3eec9c61784ae3018733c96782d16ea9fbf62e7e90acb20c6e208\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.17.0-cp37-cp37m-linux_x86_64.whl size=3388671 sha256=bad1508fd0583fe3c081d5f03de7fced1222fd1fe551265c4c9e41cbfbba9972\n",
            "  Stored in directory: /root/.cache/pip/wheels/1c/28/7e/287c6b19f7161bb03c6986a3c46b51d0d7d9a1805346634e3a\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5580 sha256=ae7d8099d5d8b3d813bb730fb28e6e72f5925585782ffa868b735a08de9aed29\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/c3/77/a5f48aeb0d3efb7cd5ad61cbd3da30bbf9ffc9662b07c9f879\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=2fedb6aa1d996f964351b3958ef9c70dacab6bc28f01d9bc7ad9f32a7cba00e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "Successfully built overrides jsonnet word2number ftfy\n",
            "Installing collected packages: urllib3, jmespath, botocore, torch, tokenizers, sacremoses, s3transfer, filelock, transformers, torchvision, tensorboardX, sentencepiece, overrides, jsonpickle, jsonnet, boto3, word2number, py-rouge, ftfy, conllu, allennlp, allennlp-models\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.4.0\n",
            "    Uninstalling filelock-3.4.0:\n",
            "      Successfully uninstalled filelock-3.4.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed allennlp-2.1.0 allennlp-models-2.1.0 boto3-1.20.24 botocore-1.23.24 conllu-4.4 filelock-3.0.12 ftfy-6.0.3 jmespath-0.10.0 jsonnet-0.17.0 jsonpickle-2.0.0 overrides-3.1.0 py-rouge-1.1 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 tensorboardX-2.4.1 tokenizers-0.10.3 torch-1.7.1 torchvision-0.8.2 transformers-4.3.3 urllib3-1.25.11 word2number-1.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install allennlp==2.1.0 allennlp-models==2.1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5INZ1yMBFt9I"
      },
      "source": [
        "#### Import predictor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wS0vAdN3FwUX",
        "outputId": "411e7d01-d065-4399-9ad6-3dd1f96e0c92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Plugin allennlp_models could not be loaded: No module named 'nltk.translate.meteor_score'\n",
            "downloading: 100%|##########| 365340283/365340283 [00:12<00:00, 29444353.74B/s]\n",
            "downloading: 100%|##########| 336/336 [00:00<00:00, 209808.86B/s]\n",
            "downloading: 100%|##########| 374434792/374434792 [00:09<00:00, 41551025.41B/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:435: UserWarning: Setting attributes on ParameterList is not supported.\n",
            "  warnings.warn(\"Setting attributes on ParameterList is not supported.\")\n"
          ]
        }
      ],
      "source": [
        "from allennlp.predictors.predictor import Predictor\n",
        "import allennlp_models.tagging\n",
        "\n",
        "predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/ner-elmo.2021-02-12.tar.gz\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predict a sentence"
      ],
      "metadata": {
        "id": "dQKEECZgJHx9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGUFr_GCGDu9"
      },
      "outputs": [],
      "source": [
        "sent = \"Semifinals ( on Saturday ) : Fung Permadi ( Taiwan ) v Indra\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gmc4DtVoF1O6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cad1059-245f-4c94-b11d-7ba70802cf6d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'logits': [[16.286212921142578,\n",
              "   -4.563837051391602,\n",
              "   -7.128006458282471,\n",
              "   -5.489968299865723,\n",
              "   -5.880529403686523,\n",
              "   -2.1837077140808105,\n",
              "   -4.072543621063232,\n",
              "   -2.2060577869415283,\n",
              "   -8.404197692871094,\n",
              "   -4.569406986236572,\n",
              "   -4.972545623779297,\n",
              "   -3.710078716278076,\n",
              "   -2.556910514831543,\n",
              "   -0.36992084980010986,\n",
              "   -5.146357536315918,\n",
              "   -5.895048141479492,\n",
              "   -6.8991241455078125],\n",
              "  [14.391697883605957,\n",
              "   -3.2945897579193115,\n",
              "   -4.361648082733154,\n",
              "   -4.4662322998046875,\n",
              "   -6.509047985076904,\n",
              "   -4.921853542327881,\n",
              "   -2.665782928466797,\n",
              "   -2.3096179962158203,\n",
              "   -5.153203964233398,\n",
              "   -0.39789843559265137,\n",
              "   -3.3818612098693848,\n",
              "   -2.260902166366577,\n",
              "   -3.040821075439453,\n",
              "   -3.254204750061035,\n",
              "   -2.0807154178619385,\n",
              "   -3.532489776611328,\n",
              "   -3.3618686199188232],\n",
              "  [14.984098434448242,\n",
              "   -0.9808083772659302,\n",
              "   -4.905792236328125,\n",
              "   -5.8497724533081055,\n",
              "   -8.0120210647583,\n",
              "   -4.564297676086426,\n",
              "   -2.7144339084625244,\n",
              "   -3.5889294147491455,\n",
              "   -4.965672492980957,\n",
              "   -1.2923029661178589,\n",
              "   -2.8974156379699707,\n",
              "   -3.0324172973632812,\n",
              "   -2.564619541168213,\n",
              "   -5.049759864807129,\n",
              "   -3.2427470684051514,\n",
              "   -4.431328773498535,\n",
              "   -4.71990966796875],\n",
              "  [17.16922378540039,\n",
              "   -3.008124351501465,\n",
              "   -6.046339988708496,\n",
              "   -6.65470027923584,\n",
              "   -8.032320022583008,\n",
              "   -4.787994384765625,\n",
              "   -4.006204605102539,\n",
              "   -2.425747871398926,\n",
              "   -6.9760541915893555,\n",
              "   -1.9447133541107178,\n",
              "   -3.893822193145752,\n",
              "   -2.7187440395355225,\n",
              "   -3.5084471702575684,\n",
              "   -3.164769172668457,\n",
              "   -3.8620872497558594,\n",
              "   -5.147453308105469,\n",
              "   -4.8716254234313965],\n",
              "  [16.242074966430664,\n",
              "   -4.588473320007324,\n",
              "   -5.034056663513184,\n",
              "   -5.555122375488281,\n",
              "   -6.083266258239746,\n",
              "   -4.822635650634766,\n",
              "   -3.2315468788146973,\n",
              "   -2.091707229614258,\n",
              "   -6.74774169921875,\n",
              "   -1.07089102268219,\n",
              "   -4.0496015548706055,\n",
              "   -2.7661654949188232,\n",
              "   -4.331987380981445,\n",
              "   -2.2898430824279785,\n",
              "   -3.449500322341919,\n",
              "   -3.7635257244110107,\n",
              "   -3.974762201309204],\n",
              "  [14.524232864379883,\n",
              "   -3.6727957725524902,\n",
              "   -1.0327669382095337,\n",
              "   -9.211883544921875,\n",
              "   -4.727288722991943,\n",
              "   -3.2217750549316406,\n",
              "   -1.534355640411377,\n",
              "   -3.6266160011291504,\n",
              "   -8.822952270507812,\n",
              "   -1.3589515686035156,\n",
              "   -2.693411350250244,\n",
              "   -5.364863395690918,\n",
              "   -4.060535907745361,\n",
              "   -3.2162585258483887,\n",
              "   -4.374634265899658,\n",
              "   -2.0088729858398438,\n",
              "   -4.580388069152832],\n",
              "  [-3.1511895656585693,\n",
              "   -3.422680139541626,\n",
              "   19.74697494506836,\n",
              "   -8.061863899230957,\n",
              "   -2.9164628982543945,\n",
              "   -1.871241807937622,\n",
              "   6.409667491912842,\n",
              "   -11.497886657714844,\n",
              "   1.6436805725097656,\n",
              "   -1.22145414352417,\n",
              "   3.164512872695923,\n",
              "   -10.028557777404785,\n",
              "   2.2826554775238037,\n",
              "   -8.236414909362793,\n",
              "   -0.9803403615951538,\n",
              "   3.4793028831481934,\n",
              "   -1.3064204454421997],\n",
              "  [-0.3195692300796509,\n",
              "   -6.3592610359191895,\n",
              "   -9.580510139465332,\n",
              "   18.90207290649414,\n",
              "   -3.6093909740448,\n",
              "   -1.6433502435684204,\n",
              "   -9.786962509155273,\n",
              "   6.057617664337158,\n",
              "   0.29820775985717773,\n",
              "   -0.4445312023162842,\n",
              "   -8.62874698638916,\n",
              "   -0.17395615577697754,\n",
              "   -8.905144691467285,\n",
              "   2.507625102996826,\n",
              "   -0.86031174659729,\n",
              "   4.419686317443848,\n",
              "   -3.693540096282959],\n",
              "  [15.752761840820312,\n",
              "   -4.773281574249268,\n",
              "   -9.783742904663086,\n",
              "   -2.917632818222046,\n",
              "   -8.528995513916016,\n",
              "   -4.9052934646606445,\n",
              "   -4.674091339111328,\n",
              "   -1.2491257190704346,\n",
              "   -7.6182966232299805,\n",
              "   -0.04856284707784653,\n",
              "   -5.203540325164795,\n",
              "   -5.3359479904174805,\n",
              "   -4.86112117767334,\n",
              "   -3.5596065521240234,\n",
              "   -3.020918369293213,\n",
              "   -2.742581844329834,\n",
              "   -6.659641742706299],\n",
              "  [-0.5700415372848511,\n",
              "   19.180500030517578,\n",
              "   -5.797320365905762,\n",
              "   -6.5873613357543945,\n",
              "   -0.8777625560760498,\n",
              "   -3.4394845962524414,\n",
              "   0.3579885959625244,\n",
              "   -1.1898714303970337,\n",
              "   -4.484407424926758,\n",
              "   -5.10823917388916,\n",
              "   1.8163707256317139,\n",
              "   2.048149824142456,\n",
              "   -2.275376796722412,\n",
              "   -7.600986480712891,\n",
              "   -9.228856086730957,\n",
              "   -6.214973449707031,\n",
              "   -10.218099594116211],\n",
              "  [15.64936637878418,\n",
              "   -4.151762962341309,\n",
              "   -6.483643054962158,\n",
              "   -5.875797748565674,\n",
              "   -5.640702724456787,\n",
              "   -5.311534881591797,\n",
              "   -4.282428741455078,\n",
              "   -0.9460716247558594,\n",
              "   -6.833130836486816,\n",
              "   -1.0667133331298828,\n",
              "   -5.513498306274414,\n",
              "   -1.8589948415756226,\n",
              "   -4.722955226898193,\n",
              "   -2.6521637439727783,\n",
              "   -3.844818115234375,\n",
              "   -4.751251220703125,\n",
              "   -3.856782913208008],\n",
              "  [15.506534576416016,\n",
              "   -3.4697136878967285,\n",
              "   -2.7952969074249268,\n",
              "   -6.978719234466553,\n",
              "   -4.778343677520752,\n",
              "   -4.745848655700684,\n",
              "   -2.5986456871032715,\n",
              "   -2.141408681869507,\n",
              "   -7.637331962585449,\n",
              "   -1.7285501956939697,\n",
              "   -4.549604415893555,\n",
              "   -3.282926082611084,\n",
              "   -4.1959991455078125,\n",
              "   -3.509082317352295,\n",
              "   -3.730022668838501,\n",
              "   -3.377415418624878,\n",
              "   -5.055962085723877],\n",
              "  [-1.778005838394165,\n",
              "   -3.7651641368865967,\n",
              "   3.0932512283325195,\n",
              "   0.969602108001709,\n",
              "   8.405893325805664,\n",
              "   -3.5821373462677,\n",
              "   -0.2032926082611084,\n",
              "   -3.0376369953155518,\n",
              "   12.20755386352539,\n",
              "   -5.60714054107666,\n",
              "   -4.083982944488525,\n",
              "   -4.941886901855469,\n",
              "   -1.3269537687301636,\n",
              "   -5.65775203704834,\n",
              "   -2.873591184616089,\n",
              "   -7.957810401916504,\n",
              "   -5.463397979736328]],\n",
              " 'mask': [True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True,\n",
              "  True],\n",
              " 'tags': ['O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'O',\n",
              "  'B-PER',\n",
              "  'L-PER',\n",
              "  'O',\n",
              "  'U-LOC',\n",
              "  'O',\n",
              "  'O',\n",
              "  'U-PER'],\n",
              " 'words': ['Semifinals',\n",
              "  '(',\n",
              "  'on',\n",
              "  'Saturday',\n",
              "  ')',\n",
              "  ':',\n",
              "  'Fung',\n",
              "  'Permadi',\n",
              "  '(',\n",
              "  'Taiwan',\n",
              "  ')',\n",
              "  'v',\n",
              "  'Indra']}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "predictor.predict(sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Flair"
      ],
      "metadata": {
        "id": "AcmGrAALm3P8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install"
      ],
      "metadata": {
        "id": "CqdFDlPynH8W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git"
      ],
      "metadata": {
        "id": "WZIJp1IcnLLn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2680
        },
        "outputId": "69f536f2-27bd-45e5-855a-53d0480539a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/flairNLP/flair.git\n",
            "  Cloning https://github.com/flairNLP/flair.git to /tmp/pip-req-build-yiem_0h4\n",
            "  Running command git clone --filter=blob:none -q https://github.com/flairNLP/flair.git /tmp/pip-req-build-yiem_0h4\n",
            "  Resolved https://github.com/flairNLP/flair.git to commit acfed5e2f6df251c8bb1ce93eacbf74a615e1344\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (1.7.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (3.2.2)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "     |████████████████████████████████| 981 kB 7.6 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "     |████████████████████████████████| 788 kB 40.0 MB/s            \n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: transformers>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (4.3.3)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "     |████████████████████████████████| 1.2 MB 32.5 MB/s            \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "     |████████████████████████████████| 61 kB 587 kB/s             \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (0.8.9)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (3.6.0)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (4.2.6)\n",
            "Collecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (4.62.3)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.10.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (1.0.1)\n",
            "Requirement already satisfied: conllu>=4.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (4.4)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (6.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (2.8.2)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "     |████████████████████████████████| 19.7 MB 1.5 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (2019.12.20)\n",
            "Collecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "     |████████████████████████████████| 48 kB 6.3 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.10) (3.0.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.10) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.10) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair==0.10) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair==0.10) (1.13.3)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.10) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.10) (5.2.1)\n",
            "Collecting requests\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "     |████████████████████████████████| 62 kB 1.1 MB/s             \n",
            "\u001b[?25hCollecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from konoha<5.0.0,>=4.0.0->flair==0.10) (3.1.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.10) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.10) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.10) (0.11.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.10) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.10) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch!=1.8,>=1.5.0->flair==0.10) (3.10.0.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair==0.10) (0.10.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair==0.10) (0.0.46)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair==0.10) (21.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair==0.10) (0.2.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair==0.10) (3.13)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.10) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (1.25.11)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (2.0.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair==0.10) (7.1.2)\n",
            "Building wheels for collected packages: flair, gdown, mpld3, segtok, sqlitedict, langdetect, pptree, wikipedia-api\n",
            "  Building wheel for flair (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.10-py3-none-any.whl size=288055 sha256=55f579b3100b4e873e553b8f6f602a85f7f47a3bfaab68091156b4c679f3333e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-06m3r4xz/wheels/5b/4d/d7/b9f138537cb1717dc8b96f64c8972b7552d9b9b51296368c43\n",
            "  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9704 sha256=bcc71ac99510e093549a02ea555b3c049ee8890a53ce9cddfa46a15b72bcde2c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=ff58306fc77f2ec036eaab418a7b2cb6047fd20d02ce0df9bb6ab2be12248b43\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25030 sha256=be0300c6f9aeb4c73669e6ac815dd45ef14255fed5390f545e49e4ffa5212131\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=07b65a506e38931f31f26d7af81511d65b1d596e93f0c0badd8965d6223ea213\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=c8e0b6c1e79df37b4c9bf3d446dad889c9b555c980b105560d586dc9a3d6734e\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=e362b1df3421cb3b022baae373698b61da056c0cc3fbf45d0af2cbb03fdd264d\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13475 sha256=08dc501c150f4b1141c1ae34eeaf4a4cdb52583bb0ed886282539b7edc0a3f53\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built flair gdown mpld3 segtok sqlitedict langdetect pptree wikipedia-api\n",
            "Installing collected packages: requests, sentencepiece, importlib-metadata, wikipedia-api, sqlitedict, segtok, pptree, mpld3, more-itertools, langdetect, konoha, janome, huggingface-hub, gdown, deprecated, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: sentencepiece\n",
            "    Found existing installation: sentencepiece 0.1.96\n",
            "    Uninstalling sentencepiece-0.1.96:\n",
            "      Successfully uninstalled sentencepiece-0.1.96\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.2\n",
            "    Uninstalling importlib-metadata-4.8.2:\n",
            "      Successfully uninstalled importlib-metadata-4.8.2\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 deprecated-1.2.13 flair-0.10 gdown-3.12.2 huggingface-hub-0.2.1 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 pptree-3.1 requests-2.26.0 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 wikipedia-api-0.5.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "importlib_metadata",
                  "more_itertools",
                  "requests",
                  "sentencepiece"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4em_3j9tnCXE"
      },
      "source": [
        "#### Import tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26e3112b-ff2b-49cd-a50c-588b21177d57",
        "id": "YGhoYLcWnCXF"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "tagger = SequenceTagger.load(\"ner-pooled\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 19:25:36,484 https://nlp.informatik.hu-berlin.de/resources/models/ner-pooled/en-ner-conll03-pooled-v0.5.pt not found in cache, downloading to /tmp/tmp3cm3favp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1125470069/1125470069 [01:05<00:00, 17292763.53B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 19:26:42,181 copying /tmp/tmp3cm3favp to cache at /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 19:26:46,140 removing temp file /tmp/tmp3cm3favp\n",
            "2021-12-14 19:26:46,283 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qf9pIllnCXO"
      },
      "source": [
        "#### Predict a sentence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = Sentence('Semifinals ( on Saturday ) : Fung Permadi ( Taiwan ) v Indra')"
      ],
      "metadata": {
        "id": "95K_SuqcJQLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "846d21bf-405b-4930-cc24-6c2ba640bad0",
        "id": "oqFh9LjhnCXP"
      },
      "source": [
        "tagger.predict(sentence, all_tag_prob=True)\n",
        "entity_dict = sentence.to_dict(tag_type='ner')\n",
        "\n",
        "print('\\n{:<12}  {:}\\n'.format('Entity', 'Type(s)'))\n",
        "for entity in entity_dict['entities']:\n",
        "    print('{:<12}  {:}'.format(entity[\"text\"], str(entity[\"labels\"])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Entity        Type(s)\n",
            "\n",
            "Fung Permadi  [PER (0.9854)]\n",
            "Taiwan        [LOC (1.0)]\n",
            "Indra         [ORG (0.9221)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. Evaluate Conll-2003\n",
        "Since the two papers both use Conll-2003, the NER dataset, to train and test on. We followed [Train a Named Entity Recognition (NER) Model Using FLAIR](https://news.machinelearning.sg/posts/train_a_named_entity_recognition_model_using_flair/) to download the dataset and did some prior study on the test set of Conll-2003."
      ],
      "metadata": {
        "id": "cv929B6hr9QF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PFvPu-WfRYC",
        "outputId": "6afee50e-3f38-4b53-d684-c13810ebd9a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download the dataset"
      ],
      "metadata": {
        "id": "WFXEXBqJecFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This following cells are from [Train a Named Entity Recognition (NER) Model Using FLAIR](https://news.machinelearning.sg/posts/train_a_named_entity_recognition_model_using_flair/)"
      ],
      "metadata": {
        "id": "3rPY3FDNftvv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y19h6OM9L2tM"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "def download_file(url, output_file):\n",
        "  Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
        "  urllib.request.urlretrieve (url, output_file)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "download_file('https://raw.githubusercontent.com/ZihanWangKi/CrossWeigh/master/data/conllpp_train.txt', '/content/drive/MyDrive/IntroToMachineLearning/Conll2003/conllpp_train.txt')\n",
        "download_file('https://raw.githubusercontent.com/ZihanWangKi/CrossWeigh/master/data/conllpp_dev.txt', '/content/drive/MyDrive/IntroToMachineLearning/Conll2003/conllpp_dev.txt')\n",
        "download_file('https://raw.githubusercontent.com/ZihanWangKi/CrossWeigh/master/data/conllpp_test.txt', '/content/drive/MyDrive/IntroToMachineLearning/Conll2003/conllpp_test.txt')"
      ],
      "metadata": {
        "id": "89_Nhs0PMKfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tagLM\n",
        "After downloading the conll-2003 dataset, we use the AllenNLP API to evaluate the test set of conll. Some of the code (such as loader) is referenced from [Train a Named Entity Recognition (NER) Model Using FLAIR](https://news.machinelearning.sg/posts/train_a_named_entity_recognition_model_using_flair/)"
      ],
      "metadata": {
        "id": "am2_t0dBs9QP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hm0mlt2i7rId"
      },
      "source": [
        "from allennlp.data.dataset_readers.conll2003 import Conll2003DatasetReader\n",
        "from allennlp.data.data_loaders.simple_data_loader import SimpleDataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE67jEZ7_MpG"
      },
      "source": [
        "trainPath = '/content/drive/MyDrive/IntroToMachineLearning/Conll2003/conllpp_train.txt'\n",
        "devPath = '/content/drive/MyDrive/IntroToMachineLearning/Conll2003/conllpp_dev.txt'\n",
        "testPath = '/content/drive/MyDrive/IntroToMachineLearning/Conll2003/conllpp_test.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hoGFpK_8lBL"
      },
      "source": [
        "loader = SimpleDataLoader.from_dataset_reader(reader=Conll2003DatasetReader(), data_path=testPath, batch_size=1, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FuLyHrN8rUS"
      },
      "source": [
        "instances = [i for i in loader.iter_instances()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "outputId": "9d5b1808-9333-4bec-9da4-f1211eee0091",
        "id": "e-VX1N6pcObx"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [[len(corpus.train), len(corpus.test), len(corpus.dev)]]\n",
        "pd.DataFrame(data, columns=[\"Train\", \"Test\", \"Development\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train</th>\n",
              "      <th>Test</th>\n",
              "      <th>Development</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14987</td>\n",
              "      <td>3684</td>\n",
              "      <td>3466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Train  Test  Development\n",
              "0  14987  3684         3466"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c27bc36f-f706-4593-84a6-cf21f80782f7",
        "id": "Z9Z4JTOWcObx"
      },
      "source": [
        "label_dict = corpus.make_label_dictionary(label_type='ner')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 19:50:55,284 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14987/14987 [00:00<00:00, 21385.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 19:50:55,994 Corpus contains the labels: ner (#204567)\n",
            "2021-12-14 19:50:55,997 Created (for label 'ner') Dictionary with 10 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We focus on the sentences that contain \"PER\" in it."
      ],
      "metadata": {
        "id": "-uI55SSnkHJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ner_labels = ['O', 'B-ORG', 'B-MISC', 'B-PER', 'I-PER', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']\n",
        "not_per_labels = ['B-ORG', 'B-MISC', 'B-LOC', 'I-ORG', 'I-MISC', 'I-LOC']\n",
        "per_labels = ['B-PER', 'I-PER']"
      ],
      "metadata": {
        "id": "wz4aAWrXkuLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqGjDJQXcOby"
      },
      "source": [
        "only_per = []\n",
        "for i in instances:\n",
        "  is_only_per = False\n",
        "  for l in i['tags'].labels:\n",
        "    if l in per_labels:\n",
        "      is_only_per = True\n",
        "      break\n",
        "    # if l in not_per_labels:\n",
        "    #   is_only_per = False\n",
        "      # break\n",
        "  if is_only_per:\n",
        "    only_per.append(i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"There are \", len(only_per), \"sentenes in the test set that contain 'PER'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1BGDbCLk4X9",
        "outputId": "0a6440c6-3a90-4cae-b34e-d7069ab4c278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  1026 sentenes in the test set that contain 'PER'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following code is for testing the sentences and do some error analysis. We only evaluate the first 100 sentences for simplification."
      ],
      "metadata": {
        "id": "0IhdeQNklk7p"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsT6eGipPBgY"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4kCzzLgHyY9"
      },
      "source": [
        "def bilou2bio(tags):\n",
        "  result = []\n",
        "  for t in tags:\n",
        "    if t[0] == 'U':\n",
        "      result.append('B' + t[1:])\n",
        "    elif t[0] == 'L':\n",
        "      result.append('I' + t[1:])\n",
        "    else:\n",
        "      result.append(t)\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVsMd-3_-k51",
        "outputId": "faaeced6-8de6-4395-a8b3-3d294d9d9054"
      },
      "source": [
        "correct_dict = {}\n",
        "incorrect_dict = {}\n",
        "others_dict = {}\n",
        "for i, instance in enumerate(tqdm(only_per[:100])):\n",
        "  prediction = predictor.predict(sentence=' '.join(instance['metadata'].metadata['words']))\n",
        "  predicted_tags = prediction['tags']\n",
        "  fixed_predicted_tags = bilou2bio(predicted_tags)\n",
        "  if len(predicted_tags) != len(instance['tags'].labels):\n",
        "    others_dict[i] = (instance, fixed_predicted_tags, predicted_tags, prediction['words'])\n",
        "    continue\n",
        "  \n",
        "  if fixed_predicted_tags == instance['tags'].labels:\n",
        "    correct_dict[i] = (instance, fixed_predicted_tags, predicted_tags, prediction['words'])\n",
        "  else:\n",
        "    incorrect_dict[i] = (instance, fixed_predicted_tags, predicted_tags, prediction['words'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"There are \", len(incorrect_dict), \"incorrect sentences\")\n",
        "print(\"incorrenct: \", incorrect_dict.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "golbmcVul1yd",
        "outputId": "09027671-4ef9-493d-a2dd-252ac8c53620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  10 incorrect sentences\n",
            "incorrenct:  dict_keys([12, 50, 76, 78, 79, 84, 85, 92, 94, 96])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "by changing the index we can evaluate the incorrect sentence manually."
      ],
      "metadata": {
        "id": "fKWKBBG9mORn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_i = 96"
      ],
      "metadata": {
        "id": "LHxZniwEmEtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([(text, true, pred) for text, true, pred in zip(incorrect_dict[test_i][-1], incorrect_dict[test_i][0]['tags'].labels, incorrect_dict[test_i][1])])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x3-jEDHmSiB",
        "outputId": "102a0093-76e6-4e70-e39b-c35d85a75cb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Leading', 'O', 'O'), ('second', 'O', 'O'), ('round', 'O', 'O'), ('scores', 'O', 'O'), ('in', 'O', 'O'), ('the', 'O', 'O'), ('Zimbabwe', 'B-MISC', 'B-MISC'), ('Open', 'I-MISC', 'I-MISC'), ('at', 'O', 'O'), ('the', 'O', 'O'), ('par-72', 'O', 'O'), ('Chapman', 'B-ORG', 'B-LOC'), ('Golf', 'I-ORG', 'I-LOC'), ('Club', 'I-ORG', 'I-LOC'), ('on', 'O', 'O'), ('Friday', 'O', 'O'), ('(', 'O', 'O'), ('South', 'B-MISC', 'B-MISC'), ('African', 'I-MISC', 'I-MISC'), ('unless', 'O', 'O'), ('stated', 'O', 'O'), (')', 'O', 'O'), (':', 'O', 'O'), ('132', 'O', 'O'), ('Des', 'B-PER', 'B-PER'), ('Terblanche', 'I-PER', 'I-PER'), ('65', 'O', 'O'), ('67', 'O', 'O'), ('133', 'O', 'O'), ('Mark', 'B-PER', 'B-PER'), ('McNulty', 'I-PER', 'I-PER'), ('(', 'O', 'O'), ('Zimbabwe', 'B-LOC', 'B-LOC'), (')', 'O', 'O'), ('72', 'O', 'O'), ('61', 'O', 'O'), ('134', 'O', 'O'), ('Steve', 'B-PER', 'B-PER'), ('van', 'I-PER', 'I-PER'), ('Vuuren', 'I-PER', 'I-PER'), ('65', 'O', 'O'), ('69', 'O', 'O'), ('136', 'O', 'O'), ('Nick', 'B-PER', 'B-PER'), ('Price', 'I-PER', 'I-PER'), ('(', 'O', 'O'), ('Zimbabwe', 'B-LOC', 'B-LOC'), (')', 'O', 'O'), ('68', 'O', 'O'), ('68', 'O', 'O'), (',', 'O', 'O'), ('Justin', 'B-PER', 'B-PER'), ('Hobday', 'I-PER', 'I-PER'), ('71', 'O', 'O'), ('65', 'O', 'O'), (',', 'O', 'O')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## flair\n",
        "This part is to do error analysis on flair model. As in tagLM, we evaluate the first 100 sentences in the test set that contain \"PER\" label."
      ],
      "metadata": {
        "id": "KoXVmjLPXvp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following loading function is referenced from [Train a Named Entity Recognition (NER) Model Using FLAIR](https://news.machinelearning.sg/posts/train_a_named_entity_recognition_model_using_flair/)"
      ],
      "metadata": {
        "id": "Xanr3FT3mpE5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW3Dm4_z9N4f",
        "outputId": "1e52ef0f-6a97-4dbe-93ae-3454facc2feb"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "columns = {0: 'text', 3: 'ner'}\n",
        "corpus = ColumnCorpus('/content/drive/MyDrive/IntroToMachineLearning_workspace/conll2003/', \n",
        "                      columns,\n",
        "                      train_file='conllpp_train.txt',\n",
        "                      test_file='conllpp_test.txt',\n",
        "                      dev_file='conllpp_dev.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 20:21:20,726 Reading data from /content/drive/MyDrive/IntroToMachineLearning_workspace/conll2003\n",
            "2021-12-14 20:21:20,732 Train: /content/drive/MyDrive/IntroToMachineLearning_workspace/conll2003/conllpp_train.txt\n",
            "2021-12-14 20:21:20,733 Dev: /content/drive/MyDrive/IntroToMachineLearning_workspace/conll2003/conllpp_dev.txt\n",
            "2021-12-14 20:21:20,735 Test: /content/drive/MyDrive/IntroToMachineLearning_workspace/conll2003/conllpp_test.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "LEP8SAN49rxl",
        "outputId": "c33d7d2e-3038-4783-f629-361716ea3e93"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [[len(corpus.train), len(corpus.test), len(corpus.dev)]]\n",
        "pd.DataFrame(data, columns=[\"Train\", \"Test\", \"Development\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train</th>\n",
              "      <th>Test</th>\n",
              "      <th>Development</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14987</td>\n",
              "      <td>3684</td>\n",
              "      <td>3466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Train  Test  Development\n",
              "0  14987  3684         3466"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qGDT2yw9wIz",
        "outputId": "316a51f4-7098-43a9-b867-b0d3f1cb9393"
      },
      "source": [
        "label_dict = corpus.make_label_dictionary(label_type='ner')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 20:21:34,922 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14987/14987 [00:00<00:00, 20753.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 20:21:35,651 Corpus contains the labels: ner (#204567)\n",
            "2021-12-14 20:21:35,654 Created (for label 'ner') Dictionary with 10 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQF19ezQmKec",
        "outputId": "e722afd2-f1f1-4b57-b206-53bb0f570e3a"
      },
      "source": [
        "sentence_with_person_name_in_testset_list = []\n",
        "\n",
        "for test_sentence in corpus.test:\n",
        "  entity_dict = test_sentence.to_dict(tag_type='ner')\n",
        "  for entity in entity_dict['entities']:\n",
        "    if 'PER' in str(entity['labels']):\n",
        "      sentence_with_person_name_in_testset_list.append(test_sentence)\n",
        "      break\n",
        "\n",
        "print('The amount of sentences in test dataset: {}\\n'.format(len(sentence_with_person_name_in_testset_list)))\n",
        "i = 0\n",
        "# for element in sentence_with_person_name_in_testset_list[:5]:\n",
        "#   print('{}. '.format(i))\n",
        "#   i = i + 1\n",
        "#   for entity in element.get_spans('ner'):\n",
        "#     print(entity)\n",
        "#   print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The amount of sentences in test dataset: 1026\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNgTl0PxRqju"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zlRBT-cSVKL"
      },
      "source": [
        "def compare_two_entity_dictionary(sentence_number, test_sentence, correct_result, fail_result, pre_entity_dictionary, post_entity_dictionary):\n",
        "  if len(pre_entity_dictionary['entities']) != len(post_entity_dictionary['entities']):\n",
        "    incorrect_result.append((sentence_number, test_sentence, pre_entity_dictionary['entities'], post_entity_dictionary['entities']))\n",
        "  else:\n",
        "    for pre_entity, post_entity in zip(pre_entity_dictionary['entities'], post_entity_dictionary['entities']):\n",
        "      if pre_entity['text'] != post_entity['text'] or str(pre_entity['labels']).split(' ')[0] != str(post_entity['labels']).split(' ')[0]:\n",
        "        incorrect_result.append((sentence_number, test_sentence, pre_entity_dictionary['entities'], post_entity_dictionary['entities']))\n",
        "      else:\n",
        "        correct_result.append((sentence_number, test_sentence, pre_entity['text'], post_entity['text'], str(pre_entity['labels']), str(post_entity['labels'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3SNuidU9ekg",
        "outputId": "24191bc9-f7f0-44be-e005-0a8a4f95dfe9"
      },
      "source": [
        "correct_result = []\n",
        "incorrect_result = []\n",
        "\n",
        "for sentence_number, test_sentence in enumerate(tqdm(sentence_with_person_name_in_testset_list[:100])):\n",
        "  pre_entity_dictionary = test_sentence.to_dict(tag_type='ner')\n",
        "  tagger.predict(test_sentence)\n",
        "  post_entity_dictionary = test_sentence.to_dict(tag_type='ner')\n",
        "  compare_two_entity_dictionary(sentence_number, test_sentence, correct_result, incorrect_result, pre_entity_dictionary, post_entity_dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:48<00:00,  1.08s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7sbhjVz0ogf",
        "outputId": "644c11e1-3dfe-421f-eca2-eb32d97b369d"
      },
      "source": [
        "print(len(correct_result))\n",
        "print(len(incorrect_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "338\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "list all the incorrect ones."
      ],
      "metadata": {
        "id": "3Y2VRF9in1QJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwxK7yp1B6CB",
        "outputId": "3ec250f4-682d-4a75-cbdd-03f417f7c276"
      },
      "source": [
        "for incorrect in incorrect_result:\n",
        "  print(incorrect[0])\n",
        "  for entity in incorrect[1].get_spans('ner'):\n",
        "    print(entity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "Span [1]: \"Cuttitta\"   [− Labels: PER (1.0)]\n",
            "Span [8,9]: \"World Cup\"   [− Labels: MISC (0.9464)]\n",
            "Span [20]: \"Italy\"   [− Labels: LOC (0.9996)]\n",
            "Span [24]: \"England\"   [− Labels: LOC (0.9998)]\n",
            "76\n",
            "Span [4]: \"BAN\"   [− Labels: PER (0.8671)]\n",
            "78\n",
            "Span [1]: \"Blinker\"   [− Labels: PER (1.0)]\n",
            "Span [10]: \"FIFA\"   [− Labels: ORG (1.0)]\n",
            "Span [26]: \"Udinese\"   [− Labels: ORG (0.9998)]\n",
            "Span [32]: \"Feyenoord\"   [− Labels: ORG (0.9965)]\n",
            "81\n",
            "Span [1]: \"Leeds\"   [− Labels: ORG (0.9894)]\n",
            "Span [3]: \"England\"   [− Labels: LOC (0.9998)]\n",
            "Span [6,7]: \"Lee Bowyer\"   [− Labels: PER (0.9979)]\n",
            "Span [29]: \"McDonald\"   [− Labels: LOC (0.4019)]\n",
            "84\n",
            "Span [1]: \"Bowyer\"   [− Labels: PER (0.9998)]\n",
            "Span [7]: \"Yorkshire\"   [− Labels: ORG (0.9591)]\n",
            "Span [26]: \"Middlesbrough\"   [− Labels: ORG (0.9985)]\n",
            "94\n",
            "Span [1]: \"Barbarians\"   [− Labels: ORG (0.9187)]\n",
            "Span [5,6]: \"Tim Stimpson\"   [− Labels: PER (0.9999)]\n",
            "Span [8]: \"England\"   [− Labels: LOC (1.0)]\n",
            "Span [13,14]: \"Nigel Walker\"   [− Labels: PER (0.9999)]\n",
            "Span [16]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [21,22]: \"Allan Bateman\"   [− Labels: PER (0.9999)]\n",
            "Span [24]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [29,30]: \"Gregor Townsend\"   [− Labels: PER (0.9999)]\n",
            "Span [32]: \"Scotland\"   [− Labels: LOC (1.0)]\n",
            "Span [37,38]: \"Tony Underwood\"   [− Labels: PER (0.9999)]\n",
            "Span [40]: \"England\"   [− Labels: LOC (1.0)]\n",
            "Span [45,46]: \"Rob Andrew\"   [− Labels: PER (0.9999)]\n",
            "Span [48]: \"England\"   [− Labels: LOC (1.0)]\n",
            "Span [53,54]: \"Rob Howley\"   [− Labels: PER (0.9998)]\n",
            "Span [56]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [61,62]: \"Scott Quinnell\"   [− Labels: PER (0.999)]\n",
            "Span [64]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [69,70]: \"Neil Back\"   [− Labels: PER (0.9998)]\n",
            "Span [72]: \"England\"   [− Labels: LOC (1.0)]\n",
            "Span [77,78]: \"Dale McIntosh\"   [− Labels: PER (0.9997)]\n",
            "Span [80]: \"Pontypridd\"   [− Labels: ORG (0.6174)]\n",
            "Span [85,86]: \"Ian Jones\"   [− Labels: PER (0.9999)]\n",
            "Span [88,89]: \"New Zealand\"   [− Labels: LOC (0.9982)]\n",
            "Span [94,95]: \"Craig Quinnell\"   [− Labels: PER (0.9991)]\n",
            "Span [97]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [102,103]: \"Darren Garforth\"   [− Labels: PER (0.9998)]\n",
            "Span [105]: \"Leicester\"   [− Labels: LOC (0.5724)]\n",
            "Span [110,111]: \"Norm Hewitt\"   [− Labels: PER (0.9997)]\n",
            "Span [113,114]: \"New Zealand\"   [− Labels: LOC (0.9983)]\n",
            "Span [119,120]: \"Nick Popplewell\"   [− Labels: PER (0.9992)]\n",
            "Span [122]: \"Ireland\"   [− Labels: LOC (0.9998)]\n",
            "96\n",
            "Span [7,8]: \"Zimbabwe Open\"   [− Labels: MISC (0.9432)]\n",
            "Span [12,13,14]: \"Chapman Golf Club\"   [− Labels: LOC (0.8714)]\n",
            "Span [18,19]: \"South African\"   [− Labels: MISC (0.9323)]\n",
            "Span [25,26]: \"Des Terblanche\"   [− Labels: PER (0.9892)]\n",
            "Span [30,31]: \"Mark McNulty\"   [− Labels: PER (1.0)]\n",
            "Span [33]: \"Zimbabwe\"   [− Labels: LOC (1.0)]\n",
            "Span [38,39,40]: \"Steve van Vuuren\"   [− Labels: PER (0.9998)]\n",
            "Span [44,45]: \"Nick Price\"   [− Labels: PER (1.0)]\n",
            "Span [47]: \"Zimbabwe\"   [− Labels: LOC (1.0)]\n",
            "Span [52,53]: \"Justin Hobday\"   [− Labels: PER (0.9999)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D. Extension work\n",
        "The following code is the extension work. We used the API from the official packages but the code is all done by ourselves. We have 4 main parts:  \n",
        "1. Sentence generation: use templates and names to generate test senetences.\n",
        "2. Test tagLM: test on tagLM model.\n",
        "3. Test flair: test on flair model.\n",
        "4. Enhance flair's memory by using the sentences in *Harry Potter Book 1*: use harry potter to enhance the memory of flair model and then test the model."
      ],
      "metadata": {
        "id": "IT80_gXJnY3j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Sentence generation\n",
        "We created some sentence templates and names which can be downloaded through this link: https://drive.google.com/drive/folders/1RNIsrDbii6z9r_X1qVPMEQtjvIvUayA_?usp=sharing"
      ],
      "metadata": {
        "id": "u4G-SKCV02bC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH2dphD919rP"
      },
      "source": [
        "hard_temp_fp = '/content/drive/MyDrive/IntroToMachineLearning/hard_sentence_templates.txt'\n",
        "easy_temp_fp = '/content/drive/MyDrive/IntroToMachineLearning/easy_sentence_templates.txt'\n",
        "hard_name_fp = '/content/drive/MyDrive/IntroToMachineLearning/rare_names.txt'\n",
        "easy_name_fp = '/content/drive/MyDrive/IntroToMachineLearning/common_names.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAS3GmJa19rQ"
      },
      "source": [
        "def read_temp(fp):\n",
        "  all_string = \"\"\n",
        "  with open(fp, 'r', encoding='utf-8-sig') as f:\n",
        "    for l in f:\n",
        "      all_string += l\n",
        "  sents = all_string.split(\".\")\n",
        "  return [s.strip()+'.' for s in sents if s != '']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS3138Zo19rQ"
      },
      "source": [
        "def read_name(fp):\n",
        "  names = []\n",
        "  with open(fp, 'r', encoding='utf-8-sig') as f:\n",
        "    for l in f:\n",
        "      names.append(l.strip())\n",
        "  return names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxlSdBJ219rQ"
      },
      "source": [
        "easy_temps = read_temp(easy_temp_fp)\n",
        "hard_temps = read_temp(hard_temp_fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jlWgq6119rQ"
      },
      "source": [
        "easy_names = read_name(easy_name_fp)\n",
        "hard_names = read_name(hard_name_fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Test tagLM"
      ],
      "metadata": {
        "id": "wtsl-5Z72B1S"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOAdiPAw-19x"
      },
      "source": [
        "def predict_sent(temps, names):\n",
        "\n",
        "  def is_correct(tags, words, name):\n",
        "    for tag, word in zip(tags, words):\n",
        "      if word == name and tag == 'U-PER':\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "  correct = [] #[(sent, name, pred), ...]\n",
        "  wrong = []\n",
        "  for temp in tqdm(temps):\n",
        "    for name in names:\n",
        "      sent = temp.replace('*', name)\n",
        "      pred = predictor.predict(sent)\n",
        "      if is_correct(pred['tags'], pred['words'], name):\n",
        "        correct.append((sent, name, pred))\n",
        "      else:\n",
        "        wrong.append((sent, name, pred))\n",
        "  print(\"\")\n",
        "  print(\"correct: \" + str(len(correct)))\n",
        "  print(\"wrong: \" + str(len(wrong)))\n",
        "  print(\"precision: \" + str(len(correct)/(len(correct) + len(wrong))))\n",
        "  return correct, wrong"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9X7KKMo79EKv",
        "outputId": "cfb83602-2d2f-4045-8d85-d65a7ef2c8ed"
      },
      "source": [
        "## easy_temp_easy_name\n",
        "ee_correct, ee_wrong = predict_sent(easy_temps, easy_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:29<00:00,  9.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "correct: 285\n",
            "wrong: 15\n",
            "precision: 0.95\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vEcsZyqD68A",
        "outputId": "4d42da5a-e509-4f84-cc84-217ab94de9d4"
      },
      "source": [
        "## easy_temp_hard_name\n",
        "correct, wrong = predict_sent(easy_temps, hard_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [02:30<00:00, 10.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "correct: 279\n",
            "wrong: 21\n",
            "precision: 0.93\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxNuCAtVD7FH",
        "outputId": "f2a52972-8355-4768-f85f-ad6adb19b5d1"
      },
      "source": [
        "## hard_temp_easy_name\n",
        "correct, wrong = predict_sent(hard_temps, easy_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [04:16<00:00, 17.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "correct: 263\n",
            "wrong: 37\n",
            "precision: 0.8766666666666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9Ow3YJND7Ll",
        "outputId": "424ddcf1-fc2c-435f-d348-443082778c42"
      },
      "source": [
        "## hard_temp_hard_name\n",
        "correct, wrong = predict_sent(hard_temps, hard_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15/15 [04:14<00:00, 17.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "correct: 229\n",
            "wrong: 71\n",
            "precision: 0.7633333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Test flair"
      ],
      "metadata": {
        "id": "qaHUikHG2c8c"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7v7bf5joQpH"
      },
      "source": [
        "def get_per_prob(sent, name):\n",
        "  for t in sent.tokens:\n",
        "    if t.text == name:\n",
        "      s_per = t.get_tags_proba_dist('ner')[10]\n",
        "      assert s_per.value == 'S-PER'\n",
        "      return s_per.score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we load the model for each template and loop over the names becuase we want to refresh the memory when it comes to the same name"
      ],
      "metadata": {
        "id": "tweOOzm7vRwT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy4xcB8eVlEc"
      },
      "source": [
        "def before_memory(temp, names, correct, wrong):\n",
        "  tagger = SequenceTagger.load(\"ner-pooled\")\n",
        "  for name in names:\n",
        "    sent_str = temp.replace('*', name)\n",
        "    sent = Sentence(sent_str)\n",
        "    tagger.predict(sent, all_tag_prob=True)\n",
        "    entities = sent.get_spans('ner')\n",
        "    sPER_score = get_per_prob(sent, name)\n",
        "    if is_correct(entities, name):\n",
        "      correct.append((sent, name, sPER_score))\n",
        "    else:\n",
        "      wrong.append((sent, name, sPER_score))\n",
        "  # print(\"\")\n",
        "  # print(\"correct: \" + str(len(correct)))\n",
        "  # print(\"wrong: \" + str(len(wrong)))\n",
        "  # print(\"precision: \" + str(len(correct)/(len(correct) + len(wrong))))\n",
        "  return correct, wrong "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNpaXB8ErbML"
      },
      "source": [
        "def evaluate(correct, wrong):\n",
        "  def mean_prob(outs):\n",
        "    total_score = 0\n",
        "    for sent, name, score in outs:\n",
        "      total_score += score\n",
        "    return total_score/len(outs)\n",
        "\n",
        "  print(\"correct: \" + str(len(correct)))\n",
        "  print(\"wrong: \" + str(len(wrong)))\n",
        "  print(\"precision: \" + str(len(correct)/(len(correct) + len(wrong))))\n",
        "  print(\"---------------------\")\n",
        "  print(\"correct prob: \" + str(mean_prob(correct)))\n",
        "  print(\"wrong prob: \" + str(mean_prob(wrong)))\n",
        "  print(\"mean prob: \" + str(mean_prob(correct+wrong)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISXQvHp9ZIC4",
        "outputId": "00a99e4a-e303-4d30-9721-0453801ae36f"
      },
      "source": [
        "print('Easy sentence with easy name\\n')\n",
        "correct = []\n",
        "wrong = []\n",
        "for template in easy_temps:\n",
        "  correct, wrong = before_memory(template, easy_names, correct, wrong)\n",
        "\n",
        "evaluate(correct, wrong)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Easy sentence with easy name\n",
            "\n",
            "2021-12-14 21:00:29,274 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:00:47,517 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:01:15,294 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:01:58,614 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:02:20,442 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:02:44,058 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:03:07,479 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:03:26,017 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:03:54,842 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:04:15,623 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:04:33,793 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:04:51,482 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:05:08,990 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:05:32,483 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:05:52,142 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "correct: 295\n",
            "wrong: 5\n",
            "precision: 0.9833333333333333\n",
            "---------------------\n",
            "correct prob: 0.989703665951551\n",
            "wrong prob: 0.1804019592702389\n",
            "mean prob: 0.9762153041735292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dL3UnVSzIN_n",
        "outputId": "68ce8d3c-bd08-4f0e-8fd5-9f4c301b14db"
      },
      "source": [
        "print('Easy sentence with hard name\\n')\n",
        "correct = []\n",
        "wrong = []\n",
        "for template in easy_temps:\n",
        "  correct, wrong = before_memory(template, hard_names, correct, wrong)\n",
        "\n",
        "evaluate(correct, wrong)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Easy sentence with hard name\n",
            "\n",
            "2021-12-14 21:07:05,383 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:07:22,152 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:07:46,618 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:08:30,436 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:08:51,718 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:09:15,023 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:09:36,392 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:09:56,765 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:10:25,299 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:10:46,129 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:11:05,038 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:11:26,833 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:11:43,472 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:12:05,456 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:12:27,740 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "correct: 288\n",
            "wrong: 12\n",
            "precision: 0.96\n",
            "---------------------\n",
            "correct prob: 0.9844548153794475\n",
            "wrong prob: 0.08512227248866111\n",
            "mean prob: 0.948481513663816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D4jQXauIat5",
        "outputId": "9feda248-0659-47bb-cc03-a622d9da0593"
      },
      "source": [
        "print('Hard sentence with easy name\\n')\n",
        "correct = []\n",
        "wrong = []\n",
        "for template in hard_temps:\n",
        "  correct, wrong = before_memory(template, easy_names, correct, wrong)\n",
        "\n",
        "evaluate(correct, wrong)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard sentence with easy name\n",
            "\n",
            "2021-12-14 21:13:01,476 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:13:21,806 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:13:46,746 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:14:07,916 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:14:26,875 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:14:47,033 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:15:10,756 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:16:00,205 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:16:34,203 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:17:04,420 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:17:46,111 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:18:38,066 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:19:24,622 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:20:26,149 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:21:21,984 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "correct: 288\n",
            "wrong: 12\n",
            "precision: 0.96\n",
            "---------------------\n",
            "correct prob: 0.9307379950017902\n",
            "wrong prob: 0.15437924101327857\n",
            "mean prob: 0.8996836448422497\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uW7j_yorIlGp",
        "outputId": "4272ac35-7611-4df1-d4ba-160586e13532"
      },
      "source": [
        "print('Hard sentence with hard name\\n')\n",
        "correct = []\n",
        "wrong = []\n",
        "for template in hard_temps:\n",
        "  correct, wrong = before_memory(template, hard_names, correct, wrong)\n",
        "\n",
        "evaluate(correct, wrong)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hard sentence with hard name\n",
            "\n",
            "2021-12-14 21:21:59,216 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:22:18,766 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:22:43,776 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:23:04,661 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:23:23,181 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:23:41,642 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:24:04,606 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:24:52,404 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:25:27,431 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:25:55,891 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:26:36,990 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:27:26,712 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:28:09,651 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:29:08,960 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:30:06,910 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "correct: 229\n",
            "wrong: 71\n",
            "precision: 0.7633333333333333\n",
            "---------------------\n",
            "correct prob: 0.9326109188612892\n",
            "wrong prob: 0.14487940514243772\n",
            "mean prob: 0.746181127281161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Enhance flair's memory by using the sentences in *Harry Potter Book 1*\n",
        "\n",
        "We downloaded *Harry Potter Book 1* from [here](https://archive.org/stream/joannek.rowlingharrypotterbook1harrypotterandthephilosophersstoneenglishonlineclub.com/Joanne%20K.%20Rowling%20%28Harry%20Potter%2C%20Book%201%29%20-%20Harry%20Potter%20and%20the%20Philosophers%20Stone%20%5BEnglishOnlineClub.com%5D_djvu.txt), and then replace \"Harry Potter\", \"Harry\", \"Potter\" with our target name. After that, we predict those sentences to let the flair model update it's memory for our target name. In the end, we test the sentences with the memory-updated model."
      ],
      "metadata": {
        "id": "7glert9h2z4S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the following experiments, we only use 5 hard names to make the experiments faster."
      ],
      "metadata": {
        "id": "XNmcQEKGx_jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hard_names5 = read_name(hard_name_fp)[:5] "
      ],
      "metadata": {
        "id": "-8g5brUpyJ3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpHxDMXmUrDs"
      },
      "source": [
        "### Before update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewRfH2FXbAjw",
        "outputId": "bc5e8a4b-2c3f-4c50-957a-07437431c4d6"
      },
      "source": [
        "# hard temp hard name\n",
        "hh_correct = []\n",
        "hh_wrong = []\n",
        "for temp in hard_temps:\n",
        "  hh_correct, hh_wrong = before_memory(temp, hard_names5, hh_correct, hh_wrong)\n",
        "evaluate(hh_correct, hh_wrong)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 21:35:30,007 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:35:39,239 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:35:51,113 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:35:59,225 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:36:08,915 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:36:17,674 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:36:26,148 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:36:43,069 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:36:54,444 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:37:06,512 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:37:20,637 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:37:35,781 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:37:54,898 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:38:14,024 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-14 21:38:33,258 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "correct: 56\n",
            "wrong: 19\n",
            "precision: 0.7466666666666667\n",
            "---------------------\n",
            "correct prob: 0.9449388895715986\n",
            "wrong prob: 0.14233771650316684\n",
            "mean prob: 0.7416132590609292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBtg0qvQcD3x"
      },
      "source": [
        "### Load *Harry Potter Book 1*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PakPc9POcHJu"
      },
      "source": [
        "# read harry potter\n",
        "def read_harry(fp):\n",
        "  def is_harry(sent):\n",
        "    if \"Harry Potter\" in sent:\n",
        "      sent = sent.replace(\"Harry Potter\", '*')\n",
        "      return True, sent\n",
        "    elif \"Harry\" in sent:\n",
        "      sent = sent.replace(\"Harry\", '*')\n",
        "      return True, sent\n",
        "    elif \"Potter\" in sent:\n",
        "      sent = sent.replace(\"Potter\", '*')\n",
        "      return True, sent\n",
        "    else:\n",
        "      return False, sent \n",
        "\n",
        "  char_remove = [',', ';', \"'s\", '@', '&', \n",
        "\t                '*', '(', ')', '#', '!', \n",
        "\t\t\t\t\t\t\t\t\t'%', '=', '+', '-', '_', \n",
        "\t\t\t\t\t\t\t\t\t':', '\"', \"'\"]\n",
        "\n",
        "  lines = []\n",
        "  with open(fp, 'r') as f:\n",
        "    for l in f:\n",
        "      l = l.strip()\n",
        "      for c in char_remove:\n",
        "        sent = l.replace(c, \"\")\n",
        "      is_contain, sent = is_harry(sent)\n",
        "      if is_contain:\n",
        "        if len(sent) != 1:\n",
        "          lines.append(sent)\n",
        "  return lines  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyKg1u96dDhS"
      },
      "source": [
        "harry_fp = '/content/drive/MyDrive/IntroToMachineLearning/Harry Potter and the Philosopher\\'s Stone.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlu1iELYiwuR"
      },
      "source": [
        "harry_temps = read_harry(harry_fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdGq7pZukIMw",
        "outputId": "a81914c8-1bbd-4634-efc7-749aa882b1dd"
      },
      "source": [
        "len(harry_temps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1366"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3EywXlSyLG-"
      },
      "source": [
        "### Update memory and test the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXAbotZP1wrQ"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qgsRhwj14_r"
      },
      "source": [
        "random.shuffle(harry_temps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAU5_h-30Xxx"
      },
      "source": [
        "def update_memory(sents, tagger):\n",
        "  for s in tqdm(sents):\n",
        "    tagger.predict(Sentence(s))\n",
        "  return tagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "we recorded the performance after 100-sentence updates and 500-sentence updates"
      ],
      "metadata": {
        "id": "psZ5tKK04_9y"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViRTii_OyKb7"
      },
      "source": [
        "def after_memory(temps, name, harry_temps):\n",
        "  def eval(tagger, temps, name):\n",
        "    correct = []\n",
        "    wrong = []\n",
        "    for temp in temps:\n",
        "      sent_str = temp.replace('*', name)\n",
        "      sent = Sentence(sent_str)\n",
        "      tagger.predict(sent, all_tag_prob=True)\n",
        "      entities = sent.get_spans('ner')\n",
        "      sPER_score = get_per_prob(sent, name)\n",
        "      if is_correct(entities, name):\n",
        "        correct.append((sent, name, sPER_score))\n",
        "      else:\n",
        "        wrong.append((sent, name, sPER_score))\n",
        "    return correct, wrong\n",
        "\n",
        "  correct_100 = []\n",
        "  wrong_100 = []\n",
        "  correct_500 = []\n",
        "  wrong_500 = []\n",
        "\n",
        "  tagger = SequenceTagger.load(\"ner-pooled\")\n",
        "  \n",
        "  ## update memory\n",
        "  harry_sents = [temp.replace('*', name) for temp in harry_temps]\n",
        "  tagger = update_memory(harry_sents[:100], tagger)\n",
        "  correct_100, wrong_100 = eval(tagger, temps, name)\n",
        "  tagger = update_memory(harry_sents[100:500], tagger)\n",
        "  correct_500, wrong_500 = eval(tagger, temps, name)\n",
        "  \n",
        "  return correct_100, wrong_100, correct_500, wrong_500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell will take about an hour"
      ],
      "metadata": {
        "id": "UAKGmGKm8FlX"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whHVENu7EMQ4",
        "outputId": "2bd0fc4a-8c9d-4405-c433-814833b25eee"
      },
      "source": [
        "# hard temp hard name\n",
        "hh_correct_100_all = []\n",
        "hh_wrong_100_all = []\n",
        "hh_correct_500_all = []\n",
        "hh_wrong_500_all = []\n",
        "for name in hard_names5:\n",
        "  hh_correct_100, hh_wrong_100, hh_correct_500, hh_wrong_500 = after_memory(hard_temps, name, harry_temps)\n",
        "  hh_correct_100_all += hh_correct_100\n",
        "  hh_wrong_100_all += hh_wrong_100\n",
        "  hh_correct_500_all += hh_correct_500\n",
        "  hh_wrong_500_all += hh_wrong_500\n",
        "\n",
        "# eval\n",
        "print('100')\n",
        "evaluate(hh_correct_100_all, hh_wrong_100_all)\n",
        "print('500')\n",
        "evaluate(hh_correct_500_all, hh_wrong_500_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 21:40:30,846 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:09<00:00,  1.44it/s]\n",
            "100%|██████████| 400/400 [04:34<00:00,  1.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 21:47:01,792 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
            "100%|██████████| 400/400 [04:47<00:00,  1.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 21:53:52,764 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:13<00:00,  1.36it/s]\n",
            "100%|██████████| 400/400 [04:52<00:00,  1.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 22:00:48,231 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:11<00:00,  1.39it/s]\n",
            "100%|██████████| 400/400 [04:45<00:00,  1.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-14 22:07:35,290 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:10<00:00,  1.41it/s]\n",
            "100%|██████████| 400/400 [04:40<00:00,  1.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "correct: 71\n",
            "wrong: 4\n",
            "precision: 0.9466666666666667\n",
            "---------------------\n",
            "correct prob: 0.9567482832451941\n",
            "wrong prob: 0.2448415644466877\n",
            "mean prob: 0.9187799249092737\n",
            "500\n",
            "correct: 71\n",
            "wrong: 4\n",
            "precision: 0.9466666666666667\n",
            "---------------------\n",
            "correct prob: 0.9509146780195371\n",
            "wrong prob: 0.25075511913746595\n",
            "mean prob: 0.91357283487916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dctL7OiGBttc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}