{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FlairFind.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poppingary/name-entity-recognition/blob/main/FlairFind.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFBdSLWo4fyM",
        "outputId": "f74d92b7-51e8-4e20-948f-c70c3f8b9925"
      },
      "source": [
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/flairNLP/flair.git\n",
            "  Cloning https://github.com/flairNLP/flair.git to /tmp/pip-req-build-6ysia6lg\n",
            "  Running command git clone -q https://github.com/flairNLP/flair.git /tmp/pip-req-build-6ysia6lg\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (2.8.2)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 7.8 MB/s \n",
            "\u001b[?25hCollecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 47.2 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 78.8 MB/s \n",
            "\u001b[?25hCollecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 513 kB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (2019.12.20)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (3.6.0)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.7 MB/s \n",
            "\u001b[?25hCollecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (1.10.0+cu111)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 39.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (4.2.6)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.10.tar.gz (25 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (1.0.1)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (4.62.3)\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (3.2.2)\n",
            "Collecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 41.7 MB/s \n",
            "\u001b[?25hCollecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (0.8.9)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.10) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.10) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.10) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair==0.10) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair==0.10) (1.13.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.10) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.10) (1.4.1)\n",
            "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting requests\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 904 kB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.10) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.10) (3.10.0.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.10) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.10) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.10) (0.11.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (2.0.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (2021.10.8)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.10) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.10) (1.1.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 43.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 34.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair==0.10) (21.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair==0.10) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair==0.10) (7.1.2)\n",
            "Building wheels for collected packages: flair, gdown, mpld3, overrides, segtok, sqlitedict, ftfy, langdetect, pptree, wikipedia-api\n",
            "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.10-py3-none-any.whl size=288053 sha256=6d776a381c44d26acd2cbf7127c381cc701803ab56ecf170688b5a77c2576e70\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mixnpnq4/wheels/5b/4d/d7/b9f138537cb1717dc8b96f64c8972b7552d9b9b51296368c43\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9704 sha256=ec352b0eb44f7ce41ce50e7089aaa79971bebf0d37553add5ac0d03dbe80e870\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=f411c93a77de3da3be63a160586c8fc930d9cf1826eba0bab3725632462083bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=940f5f0601a2414ee86fef805b9050875590151ce60b00c17af33d127bdfded6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25030 sha256=1c79f6e10e08d2d450aa2e682d4687bf19ca493a287363e4b530d9417d7a52fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=c1c46957e5fa7c053f1f14c3cda6c8b521add22ffbe3d9bbd51d1d5540af3d36\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=c08f4168858406d78c0269dd8090ed27cbdf1e8447e1e0a460f02aa59d58d208\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=c5b9a76881528542f71fcdf7fcd5ebee330e6b6db92ef860ff8749d40c553a4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=b4f0998bf6aac0a92ebcadce4fe9e32cab6def51bf9e68d98492253683fbf48c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13475 sha256=867050c5dba3115f748bb18eff4974ce7b896f5eb06a2fee8efd3b4937b105e2\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built flair gdown mpld3 overrides segtok sqlitedict ftfy langdetect pptree wikipedia-api\n",
            "Installing collected packages: requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, pptree, mpld3, more-itertools, langdetect, konoha, janome, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.2\n",
            "    Uninstalling importlib-metadata-4.8.2:\n",
            "      Successfully uninstalled importlib-metadata-4.8.2\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.10 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.2.1 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pptree-3.1 pyyaml-6.0 requests-2.26.0 sacremoses-0.0.46 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 transformers-4.12.5 wikipedia-api-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "id": "pW3Dm4_z9N4f",
        "outputId": "3a723911-923f-4d05-9a43-2dd7814b1cef"
      },
      "source": [
        "from flair.data import Corpus\n",
        "from flair.datasets import ColumnCorpus\n",
        "\n",
        "columns = {0: 'text', 3: 'ner'}\n",
        "corpus = ColumnCorpus('/content/drive/MyDrive/IntroToMachineLearning_workspace/conll2003/', \n",
        "                      columns,\n",
        "                      train_file='conllpp_train.txt',\n",
        "                      test_file='conllpp_test.txt',\n",
        "                      dev_file='conllpp_dev.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4188977fc1b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mflair\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mColumnCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'ner'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m corpus = ColumnCorpus('/content/drive/MyDrive/IntroToMachineLearning_workspace/conll2003/', \n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'flair'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "LEP8SAN49rxl",
        "outputId": "1fdc328a-6069-4291-fba2-e38c08d7df85"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = [[len(corpus.train), len(corpus.test), len(corpus.dev)]]\n",
        "pd.DataFrame(data, columns=[\"Train\", \"Test\", \"Development\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Train</th>\n",
              "      <th>Test</th>\n",
              "      <th>Development</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14987</td>\n",
              "      <td>3684</td>\n",
              "      <td>3466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Train  Test  Development\n",
              "0  14987  3684         3466"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qGDT2yw9wIz",
        "outputId": "913cf1e1-21c8-46ee-a7d4-0e063a49d38e"
      },
      "source": [
        "label_dict = corpus.make_label_dictionary(label_type='ner')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-06 04:20:54,152 Computing label dictionary. Progress:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14987/14987 [00:00<00:00, 15517.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-06 04:20:55,200 Corpus contains the labels: ner (#204567)\n",
            "2021-12-06 04:20:55,202 Created (for label 'ner') Dictionary with 10 tags: <unk>, O, B-ORG, B-MISC, B-PER, I-PER, B-LOC, I-ORG, I-MISC, I-LOC\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zSvFz8bvLKy8",
        "outputId": "f959588f-7488-46ac-ad99-874e3b60eb91"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "test_sentence = Sentence('Semifinals ( on Saturday ) : Fung Permadi ( Taiwan ) v Indra')\n",
        "tagger = SequenceTagger.load(\"ner-pooled\")\n",
        "tagger.predict(test_sentence, all_tag_prob=True)\n",
        "entity_dict = test_sentence.to_dict(tag_type='ner')\n",
        "\n",
        "print('\\n{:<12}  {:}\\n'.format('Entity', 'Type(s)'))\n",
        "for entity in entity_dict['entities']:\n",
        "    print('{:<12}  {:}'.format(entity[\"text\"], str(entity[\"labels\"])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-06 04:20:55,774 https://nlp.informatik.hu-berlin.de/resources/models/ner-pooled/en-ner-conll03-pooled-v0.5.pt not found in cache, downloading to /tmp/tmpwbikhrer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1125470069/1125470069 [00:44<00:00, 25376586.66B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-06 04:21:40,578 copying /tmp/tmpwbikhrer to cache at /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-06 04:21:44,743 removing temp file /tmp/tmpwbikhrer\n",
            "2021-12-06 04:21:44,913 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "\n",
            "Entity        Type(s)\n",
            "\n",
            "Fung Permadi  [PER (0.9854)]\n",
            "Taiwan        [LOC (1.0)]\n",
            "Indra         [ORG (0.9221)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9UzsuvcJ1iu",
        "outputId": "1716ac6f-e6be-4aa7-d1fa-ee4168df19dc"
      },
      "source": [
        "test_sentence.tokens[12].get_tags_proba_dist('ner')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unk> (0.0),\n",
              " O (0.0),\n",
              " S-ORG (0.9221),\n",
              " S-MISC (0.0001),\n",
              " B-PER (0.0015),\n",
              " E-PER (0.0),\n",
              " S-LOC (0.0003),\n",
              " B-ORG (0.0163),\n",
              " E-ORG (0.0),\n",
              " I-PER (0.0),\n",
              " S-PER (0.0592),\n",
              " B-MISC (0.0003),\n",
              " I-MISC (0.0),\n",
              " E-MISC (0.0),\n",
              " I-ORG (0.0),\n",
              " B-LOC (0.0001),\n",
              " E-LOC (0.0),\n",
              " I-LOC (0.0),\n",
              " <START> (0.0),\n",
              " <STOP> (0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TnMsrrwLk03"
      },
      "source": [
        "def text_to_sentences(file_path):\n",
        "\ttext_content = open(file_path, \"r\")\n",
        "\ttext_string = text_content.read().replace('\\n', '')\n",
        "\ttext_content.close()\n",
        "\n",
        "\tcharacters_to_remove = [',', ';', \"'s\", '@', '&', \n",
        "\t                        '*', '(', ')', '#', '!', \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t'%', '=', '+', '-', '_', \n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t':', '\"', \"'\"]\n",
        "\tfor item in characters_to_remove:\n",
        "\t\ttext_string = text_string.replace(item, '')\n",
        "\n",
        "\tcharacters_to_replace = ['?']\n",
        "\tfor item in characters_to_replace:\n",
        "\t\ttext_string = text_string.replace(item, '.')\n",
        "\n",
        "\tfirst_name = 'Indra'\n",
        "\tfirst_name_to_be_replaced = ['Harry']\n",
        "\tfor item in first_name_to_be_replaced:\n",
        "\t\ttext_string = text_string.replace(item, first_name)\n",
        "  \n",
        "\tlast_name = 'Indra'\n",
        "\tlast_name_to_be_replaced = ['Potter']\n",
        "\tfor item in last_name_to_be_replaced:\n",
        "\t\ttext_string = text_string.replace(item, last_name)\n",
        "\n",
        "\tsentences = text_string.split('.')\n",
        "\n",
        "\tj = 0\n",
        "\t\n",
        "\tfor sentence in sentences:\n",
        "\t\tif len(sentence) < 8:\n",
        "\t\t\tcontinue\n",
        "\t\telif first_name in sentence:\n",
        "\t\t\tsentences[j] = sentence\n",
        "\t\t\tj += 1\n",
        "\t\telif last_name in sentence:\n",
        "\t\t\tsentences[j] = sentence\n",
        "\t\t\tj += 1\n",
        "\n",
        "\tsentences = sentences[0:j]\n",
        "\n",
        "\treturn(sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0SpSwDtN7hG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a929ec3e-2baf-48ec-d19e-9205c5170b3e"
      },
      "source": [
        "file_path = '/content/drive/MyDrive/IntroToMachineLearning_workspace/sentence_sample/Harry Potter and the Philosopher\\'s Stone.txt'\n",
        "book_sentences = text_to_sentences(file_path)\n",
        "print('There are {} sentences which has Indra'.format(len(book_sentences)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1387 sentences which has Indra\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bByksOdwW4m4"
      },
      "source": [
        "training_sentence_predict_result = []\n",
        "\n",
        "for sentence in book_sentences:\n",
        "  training_sentence = Sentence(sentence)\n",
        "  tagger.predict(training_sentence)\n",
        "  entity_dict = training_sentence.to_dict(tag_type='ner')\n",
        "\n",
        "  for entity in entity_dict['entities']:\n",
        "    if 'Indra' in entity[\"text\"] and 'PER' in str(entity[\"labels\"]):\n",
        "      training_sentence_predict_result.append(True)\n",
        "      break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUyWAFRj8PXs",
        "outputId": "ed26931f-e894-4ed8-92b6-95d27109ce00"
      },
      "source": [
        "print('There are {} sentences predicted to PER'.format(sum(training_sentence_predict_result)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1353 sentences predicted to PER\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXzqPHNqTFtW",
        "outputId": "73b01bac-12d6-449d-eb5d-81cf9d27f353"
      },
      "source": [
        "test_sentence = Sentence('Semifinals ( on Saturday ) : Fung Permadi ( Taiwan ) v Indra')\n",
        "tagger.predict(test_sentence, all_tag_prob=True)\n",
        "entity_dict = test_sentence.to_dict(tag_type='ner')\n",
        "test_sentence.tokens[12].get_tags_proba_dist('ner')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<unk> (0.0),\n",
              " O (0.0),\n",
              " S-ORG (0.8471),\n",
              " S-MISC (0.0006),\n",
              " B-PER (0.0013),\n",
              " E-PER (0.0),\n",
              " S-LOC (0.0001),\n",
              " B-ORG (0.0665),\n",
              " E-ORG (0.0),\n",
              " I-PER (0.0),\n",
              " S-PER (0.0814),\n",
              " B-MISC (0.0029),\n",
              " I-MISC (0.0),\n",
              " E-MISC (0.0),\n",
              " I-ORG (0.0),\n",
              " B-LOC (0.0),\n",
              " E-LOC (0.0),\n",
              " I-LOC (0.0),\n",
              " <START> (0.0),\n",
              " <STOP> (0.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQF19ezQmKec",
        "outputId": "dc63b187-6e74-44d1-8d8e-2c3f8e783347"
      },
      "source": [
        "sentence_with_person_name_in_testset_list = []\n",
        "\n",
        "for test_sentence in corpus.test:\n",
        "  entity_dict = test_sentence.to_dict(tag_type='ner')\n",
        "  for entity in entity_dict['entities']:\n",
        "    if 'PER' in str(entity['labels']):\n",
        "      sentence_with_person_name_in_testset_list.append(test_sentence)\n",
        "      break\n",
        "\n",
        "print('The amount of sentences in test dataset: {}\\n'.format(len(sentence_with_person_name_in_testset_list)))\n",
        "i = 0\n",
        "for element in sentence_with_person_name_in_testset_list[:5]:\n",
        "  print('{}. '.format(i))\n",
        "  i = i + 1\n",
        "  for entity in element.get_spans('ner'):\n",
        "    print(entity)\n",
        "  print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The amount of sentences in test dataset: 1026\n",
            "\n",
            "0. \n",
            "Span [1,2]: \"Nadim Ladki\"   [− Labels: PER (1.0)]\n",
            "\n",
            "\n",
            "1. \n",
            "Span [1]: \"China\"   [− Labels: LOC (1.0)]\n",
            "Span [17]: \"Uzbek\"   [− Labels: MISC (1.0)]\n",
            "Span [19,20]: \"Igor Shkvyrin\"   [− Labels: PER (1.0)]\n",
            "Span [35]: \"Chinese\"   [− Labels: MISC (1.0)]\n",
            "\n",
            "\n",
            "2. \n",
            "Span [1,2]: \"Oleg Shatskiku\"   [− Labels: PER (1.0)]\n",
            "\n",
            "\n",
            "3. \n",
            "Span [1,2]: \"Takuya Takagi\"   [− Labels: PER (1.0)]\n",
            "Span [15,16]: \"Hiroshige Yanagimoto\"   [− Labels: PER (1.0)]\n",
            "Span [20]: \"Syrian\"   [− Labels: MISC (1.0)]\n",
            "Span [24,25]: \"Salem Bitar\"   [− Labels: PER (1.0)]\n",
            "\n",
            "\n",
            "4. \n",
            "Span [2,3]: \"Hassan Abbas\"   [− Labels: PER (1.0)]\n",
            "Span [28]: \"Bitar\"   [− Labels: PER (1.0)]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNgTl0PxRqju"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zlRBT-cSVKL"
      },
      "source": [
        "def compare_two_entity_dictionary(sentence_number, test_sentence, correct_result, fail_result, pre_entity_dictionary, post_entity_dictionary):\n",
        "  if len(pre_entity_dictionary['entities']) != len(post_entity_dictionary['entities']):\n",
        "    incorrect_result.append((sentence_number, test_sentence, pre_entity_dictionary['entities'], post_entity_dictionary['entities']))\n",
        "  else:\n",
        "    for pre_entity, post_entity in zip(pre_entity_dictionary['entities'], post_entity_dictionary['entities']):\n",
        "      if pre_entity['text'] != post_entity['text'] or str(pre_entity['labels']).split(' ')[0] != str(post_entity['labels']).split(' ')[0]:\n",
        "        incorrect_result.append((sentence_number, test_sentence, pre_entity_dictionary['entities'], post_entity_dictionary['entities']))\n",
        "      else:\n",
        "        correct_result.append((sentence_number, test_sentence, pre_entity['text'], post_entity['text'], str(pre_entity['labels']), str(post_entity['labels'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3SNuidU9ekg",
        "outputId": "832064bb-5e71-47af-bb79-74800e7422ed"
      },
      "source": [
        "correct_result = []\n",
        "incorrect_result = []\n",
        "\n",
        "for sentence_number, test_sentence in enumerate(tqdm(sentence_with_person_name_in_testset_list[:100])):\n",
        "  pre_entity_dictionary = test_sentence.to_dict(tag_type='ner')\n",
        "  tagger.predict(test_sentence)\n",
        "  post_entity_dictionary = test_sentence.to_dict(tag_type='ner')\n",
        "  compare_two_entity_dictionary(sentence_number, test_sentence, correct_result, incorrect_result, pre_entity_dictionary, post_entity_dictionary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [02:56<00:00,  1.76s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7sbhjVz0ogf",
        "outputId": "da24ac5d-fda3-4515-dad3-0af4a1b023ff"
      },
      "source": [
        "print(len(correct_result))\n",
        "print(len(incorrect_result))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "338\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGOyvYSw6S7e",
        "outputId": "76201982-342c-45fb-96d8-2c0a5b386713"
      },
      "source": [
        "correct_result[216]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(79,\n",
              " Sentence: \"Blinker was fined 75,000 Swiss francs ( $ 57,600 ) for failing to inform the Engllsh club of his previous commitment to Udinese .\"   [− Tokens: 24  − Token-Labels: \"Blinker <S-PER> was fined 75,000 Swiss <S-MISC> francs ( $ 57,600 ) for failing to inform the Engllsh <S-MISC> club of his previous commitment to Udinese <S-ORG> .\"],\n",
              " 'Blinker',\n",
              " 'Blinker',\n",
              " '[PER (1.0)]',\n",
              " '[PER (0.9999)]')"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwxK7yp1B6CB",
        "outputId": "f658e69a-4d9a-4e53-865f-8f0884c9f113"
      },
      "source": [
        "for incorrect in incorrect_result:\n",
        "  print(incorrect[0])\n",
        "  for entity in incorrect[1].get_spans('ner'):\n",
        "    print(entity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n",
            "Span [1]: \"Cuttitta\"   [− Labels: PER (1.0)]\n",
            "Span [8,9]: \"World Cup\"   [− Labels: MISC (0.9468)]\n",
            "Span [20]: \"Italy\"   [− Labels: LOC (0.9996)]\n",
            "Span [24]: \"England\"   [− Labels: LOC (0.9998)]\n",
            "76\n",
            "Span [4]: \"BAN\"   [− Labels: PER (0.8671)]\n",
            "78\n",
            "Span [1]: \"Blinker\"   [− Labels: PER (1.0)]\n",
            "Span [10]: \"FIFA\"   [− Labels: ORG (1.0)]\n",
            "Span [26]: \"Udinese\"   [− Labels: ORG (0.9998)]\n",
            "Span [32]: \"Feyenoord\"   [− Labels: ORG (0.9965)]\n",
            "81\n",
            "Span [1]: \"Leeds\"   [− Labels: ORG (0.9893)]\n",
            "Span [3]: \"England\"   [− Labels: LOC (0.9998)]\n",
            "Span [6,7]: \"Lee Bowyer\"   [− Labels: PER (0.9979)]\n",
            "Span [29]: \"McDonald\"   [− Labels: LOC (0.4028)]\n",
            "84\n",
            "Span [1]: \"Bowyer\"   [− Labels: PER (0.9998)]\n",
            "Span [7]: \"Yorkshire\"   [− Labels: ORG (0.9613)]\n",
            "Span [26]: \"Middlesbrough\"   [− Labels: ORG (0.9985)]\n",
            "94\n",
            "Span [1]: \"Barbarians\"   [− Labels: ORG (0.9187)]\n",
            "Span [5,6]: \"Tim Stimpson\"   [− Labels: PER (0.9999)]\n",
            "Span [8]: \"England\"   [− Labels: LOC (1.0)]\n",
            "Span [13,14]: \"Nigel Walker\"   [− Labels: PER (0.9999)]\n",
            "Span [16]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [21,22]: \"Allan Bateman\"   [− Labels: PER (0.9999)]\n",
            "Span [24]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [29,30]: \"Gregor Townsend\"   [− Labels: PER (0.9999)]\n",
            "Span [32]: \"Scotland\"   [− Labels: LOC (1.0)]\n",
            "Span [37,38]: \"Tony Underwood\"   [− Labels: PER (0.9999)]\n",
            "Span [40]: \"England\"   [− Labels: LOC (1.0)]\n",
            "Span [45,46]: \"Rob Andrew\"   [− Labels: PER (0.9999)]\n",
            "Span [48]: \"England\"   [− Labels: LOC (1.0)]\n",
            "Span [53,54]: \"Rob Howley\"   [− Labels: PER (0.9998)]\n",
            "Span [56]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [61,62]: \"Scott Quinnell\"   [− Labels: PER (0.999)]\n",
            "Span [64]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [69,70]: \"Neil Back\"   [− Labels: PER (0.9999)]\n",
            "Span [72]: \"England\"   [− Labels: LOC (1.0)]\n",
            "Span [77,78]: \"Dale McIntosh\"   [− Labels: PER (0.9997)]\n",
            "Span [80]: \"Pontypridd\"   [− Labels: ORG (0.6183)]\n",
            "Span [85,86]: \"Ian Jones\"   [− Labels: PER (0.9999)]\n",
            "Span [88,89]: \"New Zealand\"   [− Labels: LOC (0.9982)]\n",
            "Span [94,95]: \"Craig Quinnell\"   [− Labels: PER (0.9991)]\n",
            "Span [97]: \"Wales\"   [− Labels: LOC (0.9999)]\n",
            "Span [102,103]: \"Darren Garforth\"   [− Labels: PER (0.9998)]\n",
            "Span [105]: \"Leicester\"   [− Labels: LOC (0.5722)]\n",
            "Span [110,111]: \"Norm Hewitt\"   [− Labels: PER (0.9997)]\n",
            "Span [113,114]: \"New Zealand\"   [− Labels: LOC (0.9983)]\n",
            "Span [119,120]: \"Nick Popplewell\"   [− Labels: PER (0.9992)]\n",
            "Span [122]: \"Ireland\"   [− Labels: LOC (0.9998)]\n",
            "96\n",
            "Span [7,8]: \"Zimbabwe Open\"   [− Labels: MISC (0.942)]\n",
            "Span [12,13,14]: \"Chapman Golf Club\"   [− Labels: LOC (0.8716)]\n",
            "Span [18,19]: \"South African\"   [− Labels: MISC (0.9309)]\n",
            "Span [25,26]: \"Des Terblanche\"   [− Labels: PER (0.9899)]\n",
            "Span [30,31]: \"Mark McNulty\"   [− Labels: PER (1.0)]\n",
            "Span [33]: \"Zimbabwe\"   [− Labels: LOC (1.0)]\n",
            "Span [38,39,40]: \"Steve van Vuuren\"   [− Labels: PER (0.9998)]\n",
            "Span [44,45]: \"Nick Price\"   [− Labels: PER (1.0)]\n",
            "Span [47]: \"Zimbabwe\"   [− Labels: LOC (1.0)]\n",
            "Span [52,53]: \"Justin Hobday\"   [− Labels: PER (0.9999)]\n"
          ]
        }
      ]
    }
  ]
}