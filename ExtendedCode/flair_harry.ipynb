{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "flair_harry.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Cq2TYc__MkLY"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/poppingary/name-entity-recognition/blob/main/ExtendedCode/flair_harry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq2TYc__MkLY"
      },
      "source": [
        "## import package and load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1OTClsW_HLR3",
        "outputId": "a86f955e-6a2a-4d9e-b8cf-c13266cc5ff8"
      },
      "source": [
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/flairNLP/flair.git\n",
            "  Cloning https://github.com/flairNLP/flair.git to /tmp/pip-req-build-oyhj217z\n",
            "  Running command git clone -q https://github.com/flairNLP/flair.git /tmp/pip-req-build-oyhj217z\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (3.2.2)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (4.2.6)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (2.8.2)\n",
            "Collecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.10.tar.gz (25 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (1.10.0+cu111)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (0.8.9)\n",
            "Collecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 55.2 MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.13.0-py3-none-any.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 31.5 MB/s \n",
            "\u001b[?25hCollecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 48.4 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (2019.12.20)\n",
            "Collecting pptree\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "Requirement already satisfied: gensim>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (4.62.3)\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair==0.10) (1.0.1)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 428 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.10) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.10) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair==0.10) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair==0.10) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair==0.10) (1.13.3)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.10) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim>=3.4.0->flair==0.10) (1.4.1)\n",
            "Collecting requests\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 823 kB/s \n",
            "\u001b[?25hCollecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.10) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair==0.10) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.10) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.10) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair==0.10) (1.3.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (2.0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.10) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair==0.10) (3.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair==0.10) (21.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 49.4 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 50.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair==0.10) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair==0.10) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair==0.10) (7.1.2)\n",
            "Building wheels for collected packages: flair, gdown, mpld3, overrides, segtok, sqlitedict, ftfy, langdetect, pptree, wikipedia-api\n",
            "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.10-py3-none-any.whl size=288053 sha256=805e67bd9d73032b5f468b2c10000ae658e2eb62c18b30bc2742070566238fed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3xn8b494/wheels/5b/4d/d7/b9f138537cb1717dc8b96f64c8972b7552d9b9b51296368c43\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9704 sha256=7fdc60b8ec1bd3b3c092abc11e4956e3c95b3400bb5d715caa63d358815fed7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=4521c19557bf567813a3a912388db010bbb257a537d558d8be19736e1914a912\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=fb8a6d605f9eaa1252a322afb67867df0885feae937981aaf0221eae76028246\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25030 sha256=92274c54a4873270727a70b9a9f1ae50736ada6ff18f7327a9e37a451b33646d\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=3e7b2664ed0929e236b52927103eeea2bb87b3a18c23ddb7e4a3404e13455e40\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=9f3f3d9d7eacceded7bdadd0f3cd9207f3dfcbcfd2f952b22e799d9d051486c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=e3eb44b9ba4145d27e596b11607bd0e6dacdc97714967215490b66e0be3a4a5f\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4629 sha256=6d0b5789df87db03de73026df7d95a3aa6b2017b51a891d93fd7d984ec0b676f\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/e8/7d/a9c3c19b4722608a0d8b05a38c36bc3f230c43becd2a46794b\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13475 sha256=36eeecd08884cf5c6ba2b702e3626a901ce15fb77d1c13e5f3edaeedc0de093b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built flair gdown mpld3 overrides segtok sqlitedict ftfy langdetect pptree wikipedia-api\n",
            "Installing collected packages: requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, pptree, mpld3, more-itertools, langdetect, konoha, janome, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.2\n",
            "    Uninstalling importlib-metadata-4.8.2:\n",
            "      Successfully uninstalled importlib-metadata-4.8.2\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.12.0\n",
            "    Uninstalling more-itertools-8.12.0:\n",
            "      Successfully uninstalled more-itertools-8.12.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "markdown 3.3.6 requires importlib-metadata>=4.4; python_version < \"3.10\", but you have importlib-metadata 3.10.1 which is incompatible.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.10 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.2.1 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pptree-3.1 pyyaml-6.0 requests-2.26.0 sacremoses-0.0.46 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 transformers-4.13.0 wikipedia-api-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxpuSSurMmtp"
      },
      "source": [
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APe4ByrYMqjI",
        "outputId": "64e0762a-9883-46a2-b1a7-d2027bd75fc2"
      },
      "source": [
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "tagger = SequenceTagger.load(\"ner-pooled\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-12 23:29:25,848 https://nlp.informatik.hu-berlin.de/resources/models/ner-pooled/en-ner-conll03-pooled-v0.5.pt not found in cache, downloading to /tmp/tmpo_pfv_9t\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1125470069/1125470069 [00:38<00:00, 29001162.27B/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-12 23:30:05,006 copying /tmp/tmpo_pfv_9t to cache at /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:30:09,094 removing temp file /tmp/tmpo_pfv_9t\n",
            "2021-12-12 23:30:09,253 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQpNR4WONBm6"
      },
      "source": [
        "## load temps and names"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzT6MJkiSC-E",
        "outputId": "7f6abd90-4420-45e6-c931-d43b5c19490b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqvlbbkCNLgF"
      },
      "source": [
        "hard_temp_fp = '/content/drive/MyDrive/IntroToMachineLearning/SentenceSample/SentenceTemplate/hard_sentence_templates.txt'\n",
        "easy_temp_fp = '/content/drive/MyDrive/IntroToMachineLearning/SentenceSample/SentenceTemplate/easy_sentence_templates.txt'\n",
        "hard_name_fp = '/content/drive/MyDrive/IntroToMachineLearning/SentenceSample/NameSample/rare_names.txt'\n",
        "easy_name_fp = '/content/drive/MyDrive/IntroToMachineLearning/SentenceSample/NameSample/common_names.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1l_45xPNYza"
      },
      "source": [
        "def read_temp(fp):\n",
        "  all_string = \"\"\n",
        "  with open(fp, 'r', encoding='utf-8-sig') as f:\n",
        "    for l in f:\n",
        "      all_string += l\n",
        "  sents = all_string.split(\".\")\n",
        "  return [s.strip()+'.' for s in sents if s != '']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6eEESYWNbdd"
      },
      "source": [
        "def read_name(fp):\n",
        "  names = []\n",
        "  with open(fp, 'r', encoding='utf-8-sig') as f:\n",
        "    for l in f:\n",
        "      names.append(l.strip())\n",
        "  return names"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqTcgLYHNdSF"
      },
      "source": [
        "easy_temps = read_temp(easy_temp_fp)\n",
        "hard_temps = read_temp(hard_temp_fp)\n",
        "easy_names = read_name(easy_name_fp)[:5]\n",
        "hard_names = read_name(hard_name_fp)[:5] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpHxDMXmUrDs"
      },
      "source": [
        "## before memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uywcOSiuYiXM"
      },
      "source": [
        "def is_correct(entities, name):\n",
        "    for entity in entities:\n",
        "      if entity.text == name:\n",
        "        if entity.tag == 'PER':\n",
        "          return True\n",
        "        else:\n",
        "          return False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7v7bf5joQpH"
      },
      "source": [
        "# sent.tokens[0].get_tags_proba_dist('ner')[10].score # 10 is S-PER\n",
        "def get_per_prob(sent, name):\n",
        "  for t in sent.tokens:\n",
        "    if t.text == name:\n",
        "      s_per = t.get_tags_proba_dist('ner')[10]\n",
        "      assert s_per.value == 'S-PER'\n",
        "      return s_per.score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy4xcB8eVlEc"
      },
      "source": [
        "def before_memory(temp, names, correct, wrong):\n",
        "  tagger = SequenceTagger.load(\"ner-pooled\")\n",
        "  for name in names:\n",
        "    sent_str = temp.replace('*', name)\n",
        "    sent = Sentence(sent_str)\n",
        "    tagger.predict(sent, all_tag_prob=True)\n",
        "    entities = sent.get_spans('ner')\n",
        "    sPER_score = get_per_prob(sent, name)\n",
        "    if is_correct(entities, name):\n",
        "      correct.append((sent, name, sPER_score))\n",
        "    else:\n",
        "      wrong.append((sent, name, sPER_score))\n",
        "  # print(\"\")\n",
        "  # print(\"correct: \" + str(len(correct)))\n",
        "  # print(\"wrong: \" + str(len(wrong)))\n",
        "  # print(\"precision: \" + str(len(correct)/(len(correct) + len(wrong))))\n",
        "  return correct, wrong "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNpaXB8ErbML"
      },
      "source": [
        "def evaluate(correct, wrong):\n",
        "  def mean_prob(outs):\n",
        "    total_score = 0\n",
        "    for sent, name, score in outs:\n",
        "      total_score += score\n",
        "    return total_score/len(outs)\n",
        "\n",
        "  print(\"correct: \" + str(len(correct)))\n",
        "  print(\"wrong: \" + str(len(wrong)))\n",
        "  print(\"precision: \" + str(len(correct)/(len(correct) + len(wrong))))\n",
        "  print(\"---------------------\")\n",
        "  print(\"correct prob: \" + str(mean_prob(correct)))\n",
        "  print(\"wrong prob: \" + str(mean_prob(wrong)))\n",
        "  print(\"mean prob: \" + str(mean_prob(correct+wrong)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISXQvHp9ZIC4",
        "outputId": "3f60fc1c-9f37-4313-9257-ffa8dcb3a46d"
      },
      "source": [
        "# easy temp easy name\n",
        "ee_correct = []\n",
        "ee_wrong = []\n",
        "for temp in easy_temps:\n",
        "  ee_correct, ee_wrong = before_memory(temp, easy_names, ee_correct, ee_wrong)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-12 23:30:32,029 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:30:41,275 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:30:52,615 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:31:08,690 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:31:18,803 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:31:28,697 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:31:38,997 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:31:48,188 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:31:59,969 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:32:09,562 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:32:18,480 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:32:27,523 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:32:36,103 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:32:46,363 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:32:56,088 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjutxBaXtJIL",
        "outputId": "057393ab-6b5d-48ac-af8b-2ebfe94bd274"
      },
      "source": [
        "evaluate(ee_correct, ee_wrong)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct: 74\n",
            "wrong: 1\n",
            "precision: 0.9866666666666667\n",
            "---------------------\n",
            "correct prob: 0.9950582232024219\n",
            "wrong prob: 0.18441706895828247\n",
            "mean prob: 0.9842496744791667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOyKTfXIZpUA"
      },
      "source": [
        "# hard temp easy name\n",
        "# he_correct = []\n",
        "# he_wrong = []\n",
        "# for temp in hard_temps:\n",
        "#   he_correct, he_wrong = before_memory(temp, easy_names, he_correct, he_wrong)\n",
        "# evaluate(he_correct, he_wrong)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20IAjDn2ar70"
      },
      "source": [
        "# easy temp hard name\n",
        "# eh_correct = []\n",
        "# eh_wrong = []\n",
        "# for temp in easy_temps:\n",
        "#   eh_correct, eh_wrong = before_memory(temp, hard_names, eh_correct, eh_wrong)\n",
        "# evaluate(eh_correct, eh_wrong)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewRfH2FXbAjw",
        "outputId": "491290ee-24e5-4d21-f286-5db8c6326480"
      },
      "source": [
        "# hard temp hard name\n",
        "hh_correct = []\n",
        "hh_wrong = []\n",
        "for temp in hard_temps:\n",
        "  hh_correct, hh_wrong = before_memory(temp, hard_names, hh_correct, hh_wrong)\n",
        "evaluate(hh_correct, hh_wrong)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-12 23:33:08,719 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:33:18,597 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:33:29,187 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:33:39,297 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:33:48,101 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:33:57,074 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:34:07,739 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:34:25,878 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:34:39,549 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:34:51,464 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:35:07,410 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:35:25,711 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:35:42,121 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:36:03,861 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "2021-12-12 23:36:24,423 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n",
            "correct: 56\n",
            "wrong: 19\n",
            "precision: 0.7466666666666667\n",
            "---------------------\n",
            "correct prob: 0.9449388895715986\n",
            "wrong prob: 0.14233771650316684\n",
            "mean prob: 0.7416132590609292\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBtg0qvQcD3x"
      },
      "source": [
        "## predict harry potter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PakPc9POcHJu"
      },
      "source": [
        "# read harry potter\n",
        "def read_harry(fp):\n",
        "  def is_harry(sent):\n",
        "    if \"Harry Potter\" in sent:\n",
        "      sent = sent.replace(\"Harry Potter\", '*')\n",
        "      return True, sent\n",
        "    elif \"Harry\" in sent:\n",
        "      sent = sent.replace(\"Harry\", '*')\n",
        "      return True, sent\n",
        "    elif \"Potter\" in sent:\n",
        "      sent = sent.replace(\"Potter\", '*')\n",
        "      return True, sent\n",
        "    else:\n",
        "      return False, sent \n",
        "\n",
        "  char_remove = [',', ';', \"'s\", '@', '&', \n",
        "\t                '*', '(', ')', '#', '!', \n",
        "\t\t\t\t\t\t\t\t\t'%', '=', '+', '-', '_', \n",
        "\t\t\t\t\t\t\t\t\t':', '\"', \"'\"]\n",
        "\n",
        "  lines = []\n",
        "  with open(fp, 'r') as f:\n",
        "    for l in f:\n",
        "      l = l.strip()\n",
        "      for c in char_remove:\n",
        "        sent = l.replace(c, \"\")\n",
        "      is_contain, sent = is_harry(sent)\n",
        "      if is_contain:\n",
        "        if len(sent) != 1:\n",
        "          lines.append(sent)\n",
        "  return lines  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyKg1u96dDhS"
      },
      "source": [
        "harry_fp = '/content/drive/MyDrive/IntroToMachineLearning/SentenceSample/Harry Potter and the Philosopher\\'s Stone.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlu1iELYiwuR"
      },
      "source": [
        "harry_temps = read_harry(harry_fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdGq7pZukIMw",
        "outputId": "0ed1c394-171e-4738-c8b2-7ed84455dd65"
      },
      "source": [
        "len(harry_temps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1366"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3EywXlSyLG-"
      },
      "source": [
        "## after harry potter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXAbotZP1wrQ"
      },
      "source": [
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qgsRhwj14_r"
      },
      "source": [
        "random.shuffle(harry_temps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAU5_h-30Xxx"
      },
      "source": [
        "def update_memory(sents, tagger):\n",
        "  for s in tqdm(sents):\n",
        "    tagger.predict(Sentence(s))\n",
        "  return tagger"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViRTii_OyKb7"
      },
      "source": [
        "def after_memory(temps, name, harry_temps):\n",
        "  def eval(tagger, temps, name):\n",
        "    correct = []\n",
        "    wrong = []\n",
        "    for temp in temps:\n",
        "      sent_str = temp.replace('*', name)\n",
        "      sent = Sentence(sent_str)\n",
        "      tagger.predict(sent, all_tag_prob=True)\n",
        "      entities = sent.get_spans('ner')\n",
        "      sPER_score = get_per_prob(sent, name)\n",
        "      if is_correct(entities, name):\n",
        "        correct.append((sent, name, sPER_score))\n",
        "      else:\n",
        "        wrong.append((sent, name, sPER_score))\n",
        "    return correct, wrong\n",
        "\n",
        "  correct_500 = []\n",
        "  wrong_500 = []\n",
        "  correct_1000 = []\n",
        "  wrong_1000 = []\n",
        "\n",
        "  tagger = SequenceTagger.load(\"ner-pooled\")\n",
        "  \n",
        "  ## update memory\n",
        "  harry_sents = [temp.replace('*', name) for temp in harry_temps]\n",
        "  tagger = update_memory(harry_sents[:100], tagger)\n",
        "  correct_500, wrong_500 = eval(tagger, temps, name)\n",
        "  tagger = update_memory(harry_sents[100:500], tagger)\n",
        "  correct_1000, wrong_1000 = eval(tagger, temps, name)\n",
        "  \n",
        "  return correct_500, wrong_500, correct_1000, wrong_1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "id": "laXBO6TFCCoe",
        "outputId": "30a9e36a-8227-4539-cba3-e2d5ad72a975"
      },
      "source": [
        "# easy temp easy name\n",
        "ee_correct_500_all = []\n",
        "ee_wrong_500_all = []\n",
        "ee_correct_1000_all = []\n",
        "ee_wrong_1000_all = []\n",
        "for name in easy_names[:1]:\n",
        "  ee_correct_500, ee_wrong_500, ee_correct_1000, ee_wrong_1000 = after_memory(easy_temps, name, harry_temps)\n",
        "  ee_correct_500_all += ee_correct_500\n",
        "  ee_wrong_500_all += ee_wrong_500\n",
        "  ee_correct_1000_all += ee_correct_1000\n",
        "  ee_wrong_1000_all += ee_wrong_1000\n",
        "\n",
        "# eval\n",
        "print('500')\n",
        "evaluate(ee_correct_500_all, ee_wrong_500_all)\n",
        "print('1000')\n",
        "evaluate(ee_correct_1000_all, ee_wrong_1000_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-12 23:36:39,144 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:18<00:00,  1.28it/s]\n",
            "100%|██████████| 400/400 [05:23<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "correct: 15\n",
            "wrong: 0\n",
            "precision: 1.0\n",
            "---------------------\n",
            "correct prob: 0.9997792720794678\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-602598762e70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# eval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'500'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mee_correct_500_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mee_wrong_500_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'1000'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mee_correct_1000_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mee_wrong_1000_all\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-701cf0090bab>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(correct, wrong)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---------------------\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"correct prob: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wrong prob: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mean prob: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwrong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-701cf0090bab>\u001b[0m in \u001b[0;36mmean_prob\u001b[0;34m(outs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m       \u001b[0mtotal_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_score\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"correct: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py-0bi09bhE5"
      },
      "source": [
        "len(ee_wrong_500_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whHVENu7EMQ4",
        "outputId": "00823e0c-9f36-4633-88ed-f85747793393"
      },
      "source": [
        "# hard temp hard name\n",
        "hh_correct_500_all = []\n",
        "hh_wrong_500_all = []\n",
        "hh_correct_1000_all = []\n",
        "hh_wrong_1000_all = []\n",
        "for name in hard_names:\n",
        "  hh_correct_500, hh_wrong_500, hh_correct_1000, hh_wrong_1000 = after_memory(hard_temps, name, harry_temps)\n",
        "  hh_correct_500_all += hh_correct_500\n",
        "  hh_wrong_500_all += hh_wrong_500\n",
        "  hh_correct_1000_all += hh_correct_1000\n",
        "  hh_wrong_1000_all += hh_wrong_1000\n",
        "\n",
        "# eval\n",
        "print('500')\n",
        "evaluate(hh_correct_500_all, hh_wrong_500_all)\n",
        "print('1000')\n",
        "evaluate(hh_correct_1000_all, hh_wrong_1000_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 00:10:55,778 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:17<00:00,  1.29it/s]\n",
            "100%|██████████| 400/400 [05:19<00:00,  1.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 00:18:28,922 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:20<00:00,  1.24it/s]\n",
            "100%|██████████| 400/400 [05:36<00:00,  1.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 00:26:23,793 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:22<00:00,  1.21it/s]\n",
            "100%|██████████| 400/400 [05:39<00:00,  1.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 00:34:23,946 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:19<00:00,  1.27it/s]\n",
            "100%|██████████| 400/400 [05:27<00:00,  1.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2021-12-13 00:42:07,217 loading file /root/.flair/models/en-ner-conll03-pooled-v0.5.pt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:18<00:00,  1.28it/s]\n",
            "100%|██████████| 400/400 [05:23<00:00,  1.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "500\n",
            "correct: 70\n",
            "wrong: 5\n",
            "precision: 0.9333333333333333\n",
            "---------------------\n",
            "correct prob: 0.9666580549308232\n",
            "wrong prob: 0.23236520886421203\n",
            "mean prob: 0.9177051985263824\n",
            "1000\n",
            "correct: 71\n",
            "wrong: 4\n",
            "precision: 0.9466666666666667\n",
            "---------------------\n",
            "correct prob: 0.9606052435619731\n",
            "wrong prob: 0.25503035448491573\n",
            "mean prob: 0.9229745828111966\n"
          ]
        }
      ]
    }
  ]
}